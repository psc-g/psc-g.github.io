<!doctype html><html><head><title>In Defense of Atari - the ALE is not 'solved'!</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="In Defense of Atari"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/cme/>CME is A-OK</a></li><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/introduccion-a-transformers/>Intro a Transformers</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li><li><a href=/posts/mentoring/reviewing/>Tips for Reviewing Research Papers</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/phase1/>Phase 1</a><ul><li><a href=/posts/musicode/phase1/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/phase1/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/phase1/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/phase1/episode3/>3-Leitmotifs & Variables</a></li><li><a href=/posts/musicode/phase1/episode4/>4-Live Coding & Jazz</a></li><li><a href=/posts/musicode/phase1/episode5/>5-Repeats & Loops</a></li></ul></li><li><a href=/posts/musicode/introducing/>Introducing</a></li><li><a href=/posts/musicode/ldd/>Losses, Dissonances, and Distortions</a></li><li><a href=/posts/musicode/hallelagine/>Portrait of Hallelagine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/albums/>Albums</a></li><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/crosswords/>Crosswords</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li><li><a href=/posts/misc/pongday/>PongDay</a></li><li><a href=/posts/misc/yovoy/>yovoy</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/from_bbf_to_sss/>From BBF to SSS</a></li><li><a class=active href=/posts/research/rl/atari_defense/>In defense of Atari</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/redo/>ReDo</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li><li><a href=/posts/research/rl/sparse_rl/>SparseRL</a></li><li><a href=/posts/research/rl/precipice/>Statistical Precipice</a></li><li><a href=/posts/research/rl/tandem/>Tandem RL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/atari_defense/banner.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>December 2, 2024</p></div><div class=title><h1>In Defense of Atari - the ALE is not 'solved'!</h1></div><div class=post-content id=post-content><p>This post is based on a talk I gave at the <a href=https://autorlworkshop.github.io/>AutoRL workshop in ICML 2024</a>, which unfortunately was not recorded.</p><h2 id=introduction>Introduction</h2><p>Reinforcement Learning (RL) has been used successfully in a number of challenging tasks, such as <a href=https://deepmind.google/research/breakthroughs/alphago/>beating world champions at Go</a>, <a href=https://www.nature.com/articles/s41586-021-04301-9>controlling tokamak plasmas for nuclear fusion</a>, <a href=https://www.nature.com/articles/s41586-021-03544-w>optimized chip placement</a>, and <a href=https://www.nature.com/articles/s41586-020-2939-8>controlling stratospheric balloons</a>. All these successes have leveraged years of research and expertise and, importantly, rely on the combination of RL algorithms with deep neural networks (as proposed in the seminal <a href=https://www.nature.com/articles/nature14236>DQN paper</a>).</p><p>The use of deep networks has become the norm for most (non-theoretical) RL research papers published. One very common structure for these types of papers is the following:</p><ul><li>Motivate the problem</li><li>Define the baseline algorithm on which the work will be building on</li><li>Present some theoretical result (which almost always is limited to linear function approximators)</li><li>Introduce the paper&rsquo;s fancy new idea, building on the theory and adapted to neural nets (where the theory no longer holds)</li><li>Evaluate on a common benchmark suite (like the <a href=https://dl.acm.org/doi/abs/10.5555/2566972.2566979>Arcade Learning Environment</a>).</li><li>Aggregate scores from the suite and generate a plot that shows the new algorithm is above the baseline</li><li>Profit!</li></ul><img src=/posts/research/rl/atari_defense/new_ideas.png alt="Common way to introduce new ideas" width=80% class=center><br><p>The <a href=https://dl.acm.org/doi/abs/10.5555/2566972.2566979>Arcade Learning Environment</a> has been one of the most common benchmarks on which to play this &ldquo;profit&rdquo; game, and we have developed many algorithms that play most Atari games at superhuman levels. This has led many in the community to profess things like:</p><ul><li>&ldquo;Atari is solved&rdquo;</li><li>&ldquo;Research on the ALE is no longer interesting&rdquo;</li><li>&ldquo;Interesting paper, but I will keep my score of reject because the authors shouldn&rsquo;t focus on Atari results&rdquo;</li></ul><p>While I&rsquo;m always in favour of more experiments, I don&rsquo;t believe any of the previous statements to be true; indeed, I believe the ALE <em>can be</em> an incredibly useful research platform, as long as we&rsquo;re asking the right questions. I&rsquo;ll try to argue this in the rest of this blogpost.</p><h2 id=rl-in-the-real-world>RL in the real world</h2><p>What happens if one wants to take the latest-and-greatest RL algorithm to use in a real-world device? Can one just plug in this RL algorithm into a robot vacuum cleaner and have it avoid getting stuck on things (as mine always does)?</p><img src=/posts/research/rl/atari_defense/robot_tangled.jpg alt="Tangled robot vacuum cleaner" width=50% class=center><br><p>The answer is almost always <strong>no</strong>! In fact, none of the success stories listed above used an out-of-the-box method; they all required a team of RL researchers in order to make them work properly. Of course, one of the big reasons for the failure of &ldquo;direct transfer&rdquo; is that the environments are different (i.e. different transition and reward dynamics, different observations, different action spaces, etc.). In addition to this comes all the many variatiinos we can make to the algorithms, including:</p><ul><li>Choice of base algorithm<ul><li>Do we use value-based? Discrete? Off-policy? &mldr;</li></ul></li><li>Choice of network<ul><li>What architecture do we use? Do we use normalization? What activations and initializations? &mldr;</li></ul></li><li>Optimization choices<ul><li>What optimizer? What learning rate? Do we use a schedule? &mldr;</li></ul></li><li>Replay buffer choices<ul><li>What capacity? What sampling strategy do we use? What batch size do we use for sampling? &mldr;</li></ul></li><li>RL components<ul><li>What discount factor do we use? What update period? Do we use multi-step returns? Lambda returns? Importance sampling? &mldr;</li></ul></li><li>&mldr;</li></ul><h2 id=measuring-progress>Measuring progress</h2><p>As seen above, there are a <em>lot</em> of choices to be made. Yet, despite this, we still see progress when we directly build on prior work? In the plot below (generated with <a href=https://github.com/google/dopamine>Dopamine</a>, we can see the progress from DQN to Rainbow to IQN, as evaluated on the ALE.</p><img src=/posts/research/rl/atari_defense/algo_progress.png alt="Algorithm progress" width=70% class=center><h3 id=is-aggregate-progress-enough>Is aggregate progress enough?</h3><p>The plot above is aggregating multiple runs across multiple games using human-normalized Interquartile Mean (as we proposed in <a href=/posts/research/rl/precipice/>our paper</a>), which is a common practice in RL research papers. However, if we look at the training curves for individual games, we can see that the ordering is inconsistent.</p><img src=/posts/research/rl/atari_defense/perGameComparison.png alt="Per-game comparison" width=70% class=center><br>This inconsistency is present in pretty much any algorithm comparison out there! Thus, although aggregate results are good for providing a concise summary of performance,<blockquote><p>aggregate results can hide important per-game differences between algorithms!</p></blockquote><h3 id=are-we-always-making-progress>Are we always making progress?</h3><p>This section tells the story of how the <a href=https://papers.nips.cc/paper_files/paper/2019/hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html>DER agent</a> came about. In 2018, <a href=https://ojs.aaai.org/index.php/AAAI/article/view/11796>Hessel et al. introduced the Rainbow algorithm</a>, which combined six recent algorithmic advances into one &ldquo;mega&rdquo; agent, which was state-of-the-art for the time.</p><img src=/posts/research/rl/atari_defense/rainbow.png alt="Rainbow training curves" width=50% class=center><br><p>In 2020, <a href="https://openreview.net/forum?id=S1xCPJHtDB">Kaiser et al. introduced SimPLe</a>, a model-based method that aimed for extreme sample efficiency. As discussed in <a href=/posts/research/rl/from_bbf_to_sss/>a recent blog post</a>, the standard 200M environment frames on which DQN and Rainbow are evaluated is quite a lot of environment interactions! Kaiser et al. proposed a new benchmark which only allowed 100k agent decisions (equivalent to 400k environment frames, due to frame skipping); this benchmark was dubbed the Atari 100k benchmark and included only a subset of 26 games on which progress was achievable with 100k agent decisions. In the figure below, the authors plotted the number of agent interactions needed by Rainbow in order to match the performance of their SimPLe with only 100k interactions (red line). Clearly SimPLe is much more sample efficient!</p><img src=/posts/research/rl/atari_defense/simple.png alt="SimPLe learning efficiency" width=50% class=center><br><p>The story doesn&rsquo;t end there, though. Some of the authors of the original Rainbow paper <a href=https://papers.nips.cc/paper_files/paper/2019/hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html>published a paper</a> where they optimized the hyper-parameters of Rainbow for the Atari 100k benchmark <em>without changing anything else</em>, and demonstrated that their DER (Data-Efficient Rainbow) outperformed SimPLe on the Atari 100k benchmark!</p><img src=/posts/research/rl/atari_defense/der.png alt="DER learning curves" width=70% class=center><br><p>One could argue that the hyper-parameters of Rainbow were overly-tuned to the 200M benchmark, while the hyper-parameters of DER were overly-tuned to the 100k benchmark. More importantly, what this story highlights is that, despite careful evaluation it is quite likely that a new method <em>will not work as intended when deployed on a different environment from which it was trained on</em>, and that a significant amount of hyper-parameter tuning will be necessary. The important takeaway from this story is that:</p><blockquote><p>you cannot compare against a baseline in a new benchmark unless you also do hyper-parameter optimization for the baseline in the new benchmark!</p></blockquote><p>Amusingly, even though DER was developed <em>after</em> SimPLe, it was officially published before it (the DER authors were likely basing their comparison on a pre-print of the SimPLE paper).</p><h2 id=when-do-hyper-parameter-settings-transfer>When do hyper-parameter settings transfer?</h2><p>The story above suggests that hyper-parameter configurations are specific to the setting on which they are evaluated. In <a href="https://openreview.net/forum?id=szUyvvwoZB">a recent RLC paper</a> we investigated whether hyper-parameter configurations transfer:</p><ul><li>Across agents (between DER and <a href=https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html>DrQ($\epsilon$)</a>)</li><li>Across data regimes (between 100k and 10M environment interactions)</li><li>Across environments (using the 26 environments from the Atari 100k benchmark)</li></ul><p>To do this, we investigated 12 hyper-parameters with different values for 2 agents (DER and DrQ($\epsilon$) over 26 environments, evaluated for both 100k and 10M agent interactions, each for 5 seeds, resulting in a total of over 108k independent training runs. This results in an overwhelming number of plots to digest, so we provided <a href=https://consistent-hyperparameters.streamlit.app/>a website</a> to facilitate navigation of the full set of results. Given a set of hyper-parameter values to choose from, we were interesting in determining whether the <em>ranking</em> of those values remains consistent across the three forms of transfer mentioned above. We defined the THC metric (see <a href="https://openreview.net/forum?id=szUyvvwoZB">paper</a> for details), which provides, for each hyper-parameter, an aggregate quantification of its transferability (higher values means less transferability):</p><img src=/posts/research/rl/atari_defense/thc.png alt="THC scores" width=90% class=center><h3 id=mostly-across-agents>(Mostly) across agents</h3><p>In the figure above we can see that when evaluating transferability across agents, we generally have low THC scores, which indicates that hyper-parameter orderings are <em>mostly</em> consistent. Inspecting a few individual games confirms this, where DrQ($\epsilon$) is in the top row and DER in the bottom row.</p><img src=/posts/research/rl/atari_defense/agent_transfer.png alt="Transfer across agents" width=90% class=center><h3 id=not-across-data-regimes>Not across data regimes</h3><p>When we inspect the ordering of hyper-parameter values when evaluated on 100k versus 10M agent interactions we get far less consistency, as can ben seen by higher THC scores above and the figure below, where the ordering basically flips from one regime to the next!</p><img src=/posts/research/rl/atari_defense/data_transfer.png alt="Transfer across data regimes" width=90% class=center><h3 id=not-really-across-environments>Not really across environments</h3><p>The ranking of hyper-parameter values does not really remain consistent across environments, as evidenced by the high THC scores above. If we inspect the performance of DrQ($\epsilon$) while varying batch size on a few games, we can see that the rankings can sometimes be completely flipped (compare Asterix and Gopher versus BattleZone and Kangaroo):</p><img src=/posts/research/rl/atari_defense/game_transfer.png alt="Transfer across environments" width=90% class=center><h3 id=wakeup-call>Wakeup call!</h3><p>The above results should be a wakeup call:</p><blockquote><p>If we can’t get our agents to work reliably in Atari, what hope have we for other real-world complex systems?</p></blockquote><h2 id=why-the-ale-is-still-a-great-benchmark-for-rl-research>Why the ALE is still a great benchmark for RL research</h2><p>The last statement can be phrased differently: the ALE is a great benchmark for developing methods that are <em>consistent</em> across different training regimes, <em>robust</em> to varying hyper-parameters, without sacrificing performance. Here are some more reasons why it&rsquo;s (still) a great benchmark for general RL research.</p><h3 id=diversity-of-non-biased-environments>Diversity of &ldquo;non-biased&rdquo; environments</h3><p>The ALE suites consists of over 57 games (I normally run with 60) which vary in difficulty, reward sparsity, observational complexity, and transition dynamics. Importantly, these games were developed <em>by professional game designers for human enjoyment</em>, and <em>not</em> by RL researchers for RL research. This mitigates experimenter bias which is unfortunately present in many of the environment suites that have been developed specifically for RL research.</p><img src=/posts/research/rl/atari_defense/ale_games.png alt="ALE games" width=40% class=center><h3 id=variety-of-difficulty-modes>Variety of difficulty modes</h3><p>Many of these games also include a variety of difficulty modes which allows us to carefully investigate the generalization capabilities of our agents (read more about modes <a href=https://arxiv.org/abs/1810.00123>here</a>, and <a href="https://openreview.net/forum?id=sSt9fROSZRO">here</a>).</p><img src=/posts/research/rl/atari_defense/modes.png alt="ALE modes" width=60% class=center><h3 id=deterministic-versus-stochastic-variants>Deterministic versus stochastic variants</h3><p>The Atari games in the ALE were originally deterministic, but <a href=https://dl.acm.org/doi/10.5555/3241691.3241702>Machado et al.</a> introduced &ldquo;sticky actions&rdquo;, which causes actions to repeat (or &ldquo;stick&rdquo;) with some probability, thereby rendering the environment transitions stochastic. This can make games more/less difficult, and can avoid trivial open-loop policy solutions.</p><img src=/posts/research/rl/atari_defense/sticky.png alt="Comparison with sticky actions" width=80% class=center><h3 id=discrete-versus-continuous-control>Discrete versus continuous control</h3><p>Since its introduction, the ALE has been a suite of discrete action environments; however, the original joystick is an <em>analogue controller</em>! In our <a href="https://openreview.net/forum?id=vlUK2h1Nvw#discussion">latest NeurIPS'24 paper</a>, we introduce <a href=https://github.com/Farama-Foundation/Arcade-Learning-Environment>CALE</a>, the Continuous ALE, which enables continuous actions for the ALE. Rather than having 9 discrete positions for each joystick position (see top left circle in figure below), we parameterize the actions via three continuous dimensions: two for polar coordinates and one for the &ldquo;fire&rdquo; button. Depending on a sensitivity threshold $\tau$, the joystick position still triggers one of the 9 discrete position events.</p><img src=/posts/research/rl/atari_defense/cale.png alt="Discrete versus continuous actions in the CALE" width=80% class=center><br><p>After those events are triggered, the emulator is exactly the same for both the ALE and the CALE, which means that:</p><blockquote><p>The only difference between the ALE and the CALE is the input action space!</p></blockquote><p>The advantage of this is that we have a much more direct way to compare discrete- and continuous-action agents, without any &ldquo;actor head&rdquo; modifications! We trained <a href=https://proceedings.mlr.press/v80/haarnoja18b.html>SAC</a> and <a href=https://arxiv.org/abs/1707.06347>PPO</a> on the CALE and compared them against DQN and SAC-D (a discrete version of SAC proposed <a href=https://arxiv.org/abs/1910.07207>here</a>).</p><img src=/posts/research/rl/atari_defense/cale_comparisons.png alt="Agent comparisons on the CALE" width=80% class=center><br><p>The fact that the continuous control agents (PPO and SAC) drastically under-perform their discrete counterparts is not entirely surprising, as they have not (yet) been optimized for the CALE. What is interesting to observe is that the dominance of DQN over SAC <em>is not uniform across games</em>, which is very much consistent with the discussions above:</p><img src=/posts/research/rl/atari_defense/cale_per_game.png alt="Agent comparisons on the CALE" width=80% class=center><h3 id=other-extensions>Other extensions</h3><p>Aside from the above, there have been many interesting extensions to the ALE, a few of which I list below:</p><ul><li><a href=https://arxiv.org/abs/1903.03176>Miniature Atari (MinAtar)</a></li><li><a href=https://proceedings.mlr.press/v139/ceron21a.html>Cheaper proxies for Atari results</a></li><li><a href=https://proceedings.mlr.press/v202/aitchison23a/aitchison23a.pdf>Atari-5 (5-game representative subset)</a></li><li><a href=https://research.nvidia.com/sites/default/files/pubs/2019-07_GPU-Accelerated-Atari-Emulation/CuLE.pdf>GPU-Accelerated Atari emulation</a></li><li><a href=https://arxiv.org/abs/2203.16777>Masked observations for partial observability</a></li><li><a href=https://arxiv.org/abs/2306.08649>Object-centric ALE</a></li><li><a href=https://arxiv.org/abs/2009.09341>Multiplayer support</a></li></ul><h3 id=well-established-benchmark>Well-established benchmark</h3><p>Of course, an important point of the ALE is that it is already a well-establishedf and well-understood benchmark, which makes it easier for readers (including reviewers) to process results.</p><img src=/posts/research/rl/atari_defense/ale_citations.png alt="ALE citations" width=90% class=center><h2 id=use-the-ale-but-use-it-properly>Use the ALE, but use it properly!</h2><p>I hope this post has convinced you of why the ALE (and its extensions) is still a great benchmark for RL research. When you&rsquo;re reviewing papers and want to dock points for focusing on the ALE, please keep this post in mind. If you&rsquo;re an author and a reviewer is giving you grief about focusing on Atari, feel free to point them to this post! Of course, this assumes that you&rsquo;re asking the right questions. Here are some tips to (hopefully) help in this respect.</p><h3 id=stop-focusing-only-on-aggregate-plots>Stop focusing only on aggregate plots</h3><p>Keep track of games where your method improves and those where it degrades</p><ul><li>Are there patterns?</li><li>Does ordering change with different hparam choices?</li><li>Discuss these in your work!</li></ul><h3 id=measure-distribution-of-scores>Measure distribution of scores</h3><p>If you can, report a distribution of scores rather than a single statistic, and consider measure the consistency of your results with bootstrapping (as we did in <a href=https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html>our paper</a>).</p><img src=/posts/research/rl/atari_defense/score_distribution.png alt="Distribution of scores" width=90% class=center><h3 id=dont-exclude-games>Don’t exclude games</h3><p>For computational reasons, many works often focus their analyses on a subset of games. If you have the compute, run at least one experiment on the set of all games as this helps evaluate the generalizability of the claims. For example, in <a href="https://openreview.net/forum?id=2sjm6AH1jB">our BBF paper</a> we were mostly focused on the 26 Atari 100k games, but did run comparisons on the remaining games.</p><img src=/posts/research/rl/atari_defense/bbf_all_games.png alt="BBF evaluation on all games" width=50% class=center><h3 id=run-for-longer--shorter>Run for longer / shorter</h3><p>Try to run for different training lengths. As discussed above, changing the data regimes can have a dramatic effect on reported performance! This is something discussed in the excellent <a href=https://jmlr.org/papers/v25/23-0183.html>Empirical Design in Reinforcement Learning</a>, and something we did in <a href="https://openreview.net/forum?id=2sjm6AH1jB">our BBF paper</a>:</p><img src=/posts/research/rl/atari_defense/bbf_longer.png alt="BBF evaluated for longer" width=80% class=center><h3 id=better-characterization-of-games>Better characterization of games</h3><p>The huge variance we see across games demonstrates that we lack good, quantifiable, characterizations for these games. Most characterizations have been rather high-level and manually crafted (e.g. &ldquo;sparse reward&rdquo; games), with <a href=https://proceedings.mlr.press/v202/aitchison23a/aitchison23a.pdf>the Atari-5 paper</a> providing a bit more granularity:</p><img src=/posts/research/rl/atari_defense/atari5_clusters.png alt="Atari-5 clusters" width=70% class=center><br><p>Ideally we would have a rich enough characterizations of the various games (that are useful for general environments) that enable automatically adjusting hyper-parameter settings for each environment.</p><h2 id=conclusion>Conclusion</h2><p>If our only goal is to &ldquo;play Atari games really really well&rdquo;, then maybe we&rsquo;re done. But that was never the point of the ALE, it was always meant as a <em>research platform</em>. We should be striving to develop methods that are <em>reliable</em> and <em>transferable</em>, and this requires going beyond pure &ldquo;SotA-chasing&rdquo;, and even beyond cumulative returns, and conducting more thorough and &ldquo;nitty-gritty&rdquo; analyses to get a better sense of what makes deep RL agents tick. I think we should asking ourselves <em>why</em> our algorithms learn/behave the way they do, rather than trying to squeeze out some extra IQM points to claim SotA.</p><p>The ALE is an extremely valuable resource in our mission to develop more generally capable agents, as long as you ask the right questions and use it properly!</p><p>P.S. If you want to learn about the history of how the originally Atari 2600 console was developed, I <em>highly</em> recommend the book <a href=https://mitpress.mit.edu/9780262539760/racing-the-beam/>Racing the Beam</a>.</p><img src=/posts/research/rl/atari_defense/racingTheBeam.png alt="Racing the Beam" width=20% class=center></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/atari_defense.md><i class="fas fa-code-branch"></i>
Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/research/rl/from_bbf_to_sss/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>From "Bigger, Better, Faster" to "Smaller, Sparser, Stranger"</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="does-not-exist",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#rl-in-the-real-world>RL in the real world</a></li><li><a href=#measuring-progress>Measuring progress</a><ul><li><a href=#is-aggregate-progress-enough>Is aggregate progress enough?</a></li><li><a href=#are-we-always-making-progress>Are we always making progress?</a></li></ul></li><li><a href=#when-do-hyper-parameter-settings-transfer>When do hyper-parameter settings transfer?</a><ul><li><a href=#mostly-across-agents>(Mostly) across agents</a></li><li><a href=#not-across-data-regimes>Not across data regimes</a></li><li><a href=#not-really-across-environments>Not really across environments</a></li><li><a href=#wakeup-call>Wakeup call!</a></li></ul></li><li><a href=#why-the-ale-is-still-a-great-benchmark-for-rl-research>Why the ALE is still a great benchmark for RL research</a><ul><li><a href=#diversity-of-non-biased-environments>Diversity of &ldquo;non-biased&rdquo; environments</a></li><li><a href=#variety-of-difficulty-modes>Variety of difficulty modes</a></li><li><a href=#deterministic-versus-stochastic-variants>Deterministic versus stochastic variants</a></li><li><a href=#discrete-versus-continuous-control>Discrete versus continuous control</a></li><li><a href=#other-extensions>Other extensions</a></li><li><a href=#well-established-benchmark>Well-established benchmark</a></li></ul></li><li><a href=#use-the-ale-but-use-it-properly>Use the ALE, but use it properly!</a><ul><li><a href=#stop-focusing-only-on-aggregate-plots>Stop focusing only on aggregate plots</a></li><li><a href=#measure-distribution-of-scores>Measure distribution of scores</a></li><li><a href=#dont-exclude-games>Don’t exclude games</a></li><li><a href=#run-for-longer--shorter>Run for longer / shorter</a></li><li><a href=#better-characterization-of-games>Better characterization of games</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Selected Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul><a rel=me href=https://sigmoid.social/@psc>Mastodon</a></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad()</script></body></html>