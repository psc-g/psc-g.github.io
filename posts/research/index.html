<!doctype html><html><head><title>Research</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/assets/css/layouts/list.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-XXXXXXXXX-X','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><a href=/posts/musicode/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/episode1/>1-Musical Note & Computation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/rl/>RL</a><ul><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/research/rl/metrics_continuity/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/metrics_continuity/banner.png></div><div class=card-body><h5 class=card-title>Metrics and continuity in reinforcement learning</h5><p class="card-text post-summary">In this work we investigate the notion of &ldquo;state similarity&rdquo; in Markov decision processes. This concept is central to generalization in RL with function approximation.
Our paper was published at AAAI'21.
Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro
The text below was adapted from Charline&rsquo;s twitter thread
In RL, we often deal with systems with large state spaces. We can’t exactly represent the value of each of these states and need some type of generalization.</p></div><div class=card-footer><span class=float-left>February 3, 2021</span>
<a href=/posts/research/rl/metrics_continuity/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/pse/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/pse/pse.png></div><div class=card-body><h5 class=card-title>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning</h5><p class="card-text post-summary">This paper was accepted as a spotlight at ICLR'21.
We propose a new metric and contrastive loss that comes equipped with theoretical and empirical results.
Policy Similarity Metric We introduce the policy similarity metric (PSM) which is based on bisimulation metrics. In contrast to bisimulation metrics (which is built on reward differences), PSMs are built on differences in optimal policies.
If we were to use this metric for policy transfer (as Doina Precup & I explored previously), we can upper-bound the difference between the optimal and the transferred policy:</p></div><div class=card-footer><span class=float-left>January 14, 2021</span>
<a href=/posts/research/rl/pse/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/2020highlights/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/2020highlights/banner.png></div><div class=card-body><h5 class=card-title>2020 RL highlights</h5><p class="card-text post-summary">As part of TWiML &rsquo;s AI Rewind series, I was asked to provide a list of reinforcement learning papers that were highlights for me in 2020. It&rsquo;s been a difficult year for pretty much everyone, but it&rsquo;s heartening to see that despite all the difficulties, interesting research still came out.
Given the size and breadth of the reinforcement learning research, as well as the fact that I was asked to do this at the end of NeurIPS and right before my vacation, I decided to apply the following rules in the selection:</p></div><div class=card-footer><span class=float-left>December 16, 2020</span>
<a href=/posts/research/rl/2020highlights/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/loon/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/loon/loonAnimation.gif></div><div class=card-body><h5 class=card-title>Autonomous navigation of stratospheric balloons using reinforcement learning</h5><p class="card-text post-summary">In this work we, quite literally, take reinforcement learning to new heights! Specifically, we use deep reinforcement learning to help control the navigation of stratospheric balloons, whose purpose is to deliver internet to areas with low connectivity. This project is an ongoing collaboration with Loon.
It&rsquo;s been incredibly rewarding to see reinforcement learning deployed successfully in a real setting. It&rsquo;s also been terrific to work alongside such fantastic co-authors:
Marc G.</p></div><div class=card-footer><span class=float-left>December 2, 2020</span>
<a href=/posts/research/rl/loon/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/creativity/agence/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/creativity/agence/banner.png></div><div class=card-body><h5 class=card-title>Agence: a dynamic film exploring multi-agent systems and human agency</h5><p class="card-text post-summary">Agence is a dynamic and interactive film authored by three parties: 1) the director, who establishes the narrative structure and environment, 2) intelligent agents, using reinforcement learning or scripted (hierarchical state machines) AI, and 3) the viewer, who can interact with the system to affect the simulation. We trained RL agents in a multi-agent fashion to control some (or all, based on user choice) of the agents in the film. You can download the game at the Agence website.</p></div><div class=card-footer><span class=float-left>December 1, 2020</span>
<a href=/posts/research/creativity/agence/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/revisiting_rainbow/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/revisiting_rainbow/revisiting_rainbow.png></div><div class=card-body><h5 class=card-title>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</h5><p class="card-text post-summary">We argue for the value of small- to mid-scale environments in deep RL for increasing scientific insight and help make our community more inclusive.
Johan S. Obando-Ceron and Pablo Samuel Castro
This is a summary of our paper which will be presented in the deep reinforcement learning workshop at NeurIPS 2020.
The code is available here.
You can see the Deep RL talk here.
Introduction Since the introduction of DQN (Mnih et al.</p></div><div class=card-footer><span class=float-left>November 30, 2020</span>
<a href=/posts/research/rl/revisiting_rainbow/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/creativity/ganterpretations/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/creativity/ganterpretations/gandy.gif></div><div class=card-body><h5 class=card-title>GANterpretations</h5><p class="card-text post-summary">GANterpretations is an idea I published in this paper, which was accepted to the 4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020. The code is available here.
At a high level what it does is use the spectrogram of a piece of audio (from a video, for example) to &ldquo;draw&rdquo; a path in the latent space of a BigGAN.
The following video walks through the process:</p></div><div class=card-footer><span class=float-left>November 8, 2020</span>
<a href=/posts/research/creativity/ganterpretations/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/other/rigl/rigl/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/other/rigl/rigl.gif></div><div class=card-body><h5 class=card-title>Rigging the Lottery: Making All Tickets Winners</h5><p class="card-text post-summary">Rigging the Lottery: Making All Tickets Winners is a paper published at ICML 2020 with Utku Evci, Trevor Gale, Jacob Menick, and Erich Elsen, where we introduce an algorithm for training sparse neural networks that uses a fixed parameter count and computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods.
You can read more about it in the paper and in our blog post.</p></div><div class=card-footer><span class=float-left>September 16, 2020</span>
<a href=/posts/research/other/rigl/rigl/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/scalable/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/scalable/banner.gif></div><div class=card-body><h5 class=card-title>Scalable methods for computing state similarity in deterministic MDPs</h5><p class="card-text post-summary">This post describes my paper Scalable methods for computing state similarity in deterministic MDPs, published at AAAI 2020. The code is available here.
Motivation We consider distance metrics between states in an MDP. Take the following MDP, where the goal is to reach the green cells:
Physical distance betweent states? Physical distance often fails to capture the similarity properties we&rsquo;d like:
State abstractions Now imagine we add an exact copy of these states to the MDP (think of it as an additional &ldquo;floor&rdquo;):</p></div><div class=card-footer><span class=float-left>November 22, 2019</span>
<a href=/posts/research/rl/scalable/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/creativity/ml-jam/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/creativity/ml-jam/mljam.gif></div><div class=card-body><h5 class=card-title>ML-Jam: Performing Structured Improvisations with Pre-trained Models</h5><p class="card-text post-summary">This paper, published in the International Conference on Computational Creativity, 2019, explores using pre-trained musical generative models in a collaborative setting for improvisation.
You can read more details about it in this blog post.
You can also play with it in this web app!
If you want to play with the code, it is here.
Demos Demo video playing with the web app:
Demo video jamming over Herbie Hancock&rsquo;s Chameleon:</p></div><div class=card-footer><span class=float-left>June 19, 2019</span>
<a href=/posts/research/creativity/ml-jam/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/dopamine/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/dopamine/dopamine.png></div><div class=card-body><h5 class=card-title>Dopamine: A framework for flexible value-based reinforcement learning research</h5><p class="card-text post-summary">Dopamine is a framework for flexible, value-based, reinforcement learning research. It was originally written in TensorFlow, but now all agents have been implemented in JAX.
You can read more about it in our github page and in our white paper.
Original Google AI blogpost.
We have a website where you can easily compare the performance of all the Dopamine agents, which I find really useful:
.
We also provide a set of Colaboratory notebooks that really help understand the framework:</p></div><div class=card-footer><span class=float-left>August 27, 2018</span>
<a href=/posts/research/rl/dopamine/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=/assets/js/list.js></script></body></html>