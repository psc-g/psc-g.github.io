<!doctype html><html><head><title>MICo: Learning improved representations via sampling-based state similarity for Markov decision processes</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="MICo: Learning improved representations via sampling-based state similarity for Markov decision processes"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-XXXXXXXXX-X','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/cme/>CME is A-OK</a></li><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/introduccion-a-transformers/>Intro a Transformers</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li><li><a href=/posts/mentoring/reviewing/>Tips for Reviewing Research Papers</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/phase1/>Phase 1</a><ul><li><a href=/posts/musicode/phase1/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/phase1/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/phase1/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/phase1/episode3/>3-Leitmotifs & Variables</a></li><li><a href=/posts/musicode/phase1/episode4/>4-Live Coding & Jazz</a></li><li><a href=/posts/musicode/phase1/episode5/>5-Repeats & Loops</a></li></ul></li><li><a href=/posts/musicode/introducing/>Introducing</a></li><li><a href=/posts/musicode/ldd/>Losses, Dissonances, and Distortions</a></li><li><a href=/posts/musicode/hallelagine/>Portrait of Hallelagine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/crosswords/>Crosswords</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li><li><a href=/posts/misc/yovoy/>yovoy</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a class=active href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li><li><a href=/posts/research/rl/sparse_rl/>SparseRL</a></li><li><a href=/posts/research/rl/precipice/>Statistical Precipice</a></li><li><a href=/posts/research/rl/tandem/>Tandem RL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/mico/banner.gif)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>October 21, 2021</p></div><div class=title><h1>MICo: Learning improved representations via sampling-based state similarity for Markov decision processes</h1></div><div class=post-content id=post-content><p>We present a new behavioural distance over the state space of a Markov
decision process, and demonstrate the use of this distance as an effective
means of shaping the learnt representations of deep reinforcement learning
agents.</p><p><em>Pablo Samuel Castro*, Tyler Kastner*, Prakash Panangaden, and Mark Rowland</em></p><center><iframe width=560 height=315 src=https://www.youtube.com/embed/CWKv2R30c9E title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center><hr><p>This blogpost is a summary of our <a href=https://arxiv.org/abs/2106.08229>NeurIPS 2021 paper</a>.
The code is available
<a href=https://github.com/google-research/google-research/tree/master/mico>here</a>.</p><p>The following figure gives a nice summary of the empirical gains our new loss
provides, yielding an improvement on all of the
<a href=https://github.com/google/dopamine>Dopamine</a> agents (left), as well as over Soft
Actor-Critic and the DBC algorithm of <a href=https://arxiv.org/abs/2006.10742>Zhang et al., ICLR 2021</a> (right).
In both cases we are reporting the Interquantile Mean as introduced in
<a href=https://arxiv.org/abs/2108.13264>our Statistical Precipice NeurIPS'21 paper</a>.</p><img src=/posts/research/rl/mico/iqm.png alt=IQM width=90% class=center><h2 id=introduction>Introduction</h2><p>The success of reinforcement learning (RL) algorithms in large-scale, complex
tasks depends on forming useful representations of the environment with which
the algorithms interact. Feature selection and feature learning has long been
an important subdomain of RL, and with the advent of deep reinforcement
learning there has been much recent interest in understanding and improving
the representations learnt by RL agents.</p><p>We introduce the <em>MICo</em> (<strong>M</strong><em>atching under</em> <strong>I</strong><em>ndependent</em> <strong>Co</strong><em>uplings</em>)
<em>distance</em>, and develop the theory around its computation and estimation,
making comparisons with existing metrics on the basis of computational and
statistical efficiency. We empirically demonstrate that
<em>directly shaping the representation</em> with MICo (as opposed to implicitly as
most previous methods) yields improvements on a number of value-based deep
RL agents.</p><details><summary>Expand to read more details</summary><p>Much of the work in representation learning has taken place from the
perspective of auxiliary tasks
<a href=https://arxiv.org/abs/1611.05397>[Jaderberg et al., 2017]</a>,
<a href=https://arxiv.org/abs/1902.06865>[Fedus et al., 2019]</a>;
in addition to the primary reinforcement learning task, the agent may attempt
to predict and control additional aspects of the environment. Auxiliary tasks
shape the agent’s representation of the environment implicitly, typically via
gradient descent on the additional learning objectives. As such, while
auxiliary tasks continue to play an important role in improving the
performance of deep RL algorithms, our understanding of the effects of
auxiliary tasks on representations in RL is still in its infancy.</p><p>In contrast to the implicit representation shaping of auxiliary tasks, a
separate line of work on behavioural metrics, such as bisimulation metrics
<a href=https://link.springer.com/chapter/10.1007/3-540-48320-9_19>[Desharnais et al., 1999]</a>,
<a href=https://arxiv.org/abs/1207.4114>[Ferns et al., 2004]</a>, aims to capture
structure in the environment by learning a metric measuring behavioral
similarity between states. Recent works have successfully used behavioural
metrics to shape the representations of deep RL agents
<a href=https://arxiv.org/abs/1906.02736>[Gelada et al., 2019]</a>,
<a href=https://arxiv.org/abs/2006.10742>[Zhang et al., 2021]</a>,
<a href=https://arxiv.org/abs/2101.05265>[Agarwal et al., 2021]</a>.
However, in practice behavioural metrics are difficult to estimate from both
statistical and computational perspectives, and these works either rely on
specific assumptions about transition dynamics to make the estimation
tractable, and as such can only be applied to limited classes of
environments, or are applied to more general classes of environments not
covered by theoretical guarantees.</p><p>The principal objective of this work is to develop new measures of behavioral
similarity that avoid the statistical and computational difficulties
described above, and simultaneously capture richer information about the
environment.</p></details><h2 id=background>Background</h2><p>To streamline the reading of this post, I&rsquo;ve made the details in the background
section collapsible. Click on each section header to read more.</p><h3 id=reinforcement-learning>Reinforcement learning</h3><p>Reinforcement learning methods are used for sequential decision making in uncertain environments.
You can read an introduction to reinforcement learning in
<a href=/posts/mentoring/intro-to-rl/>this post</a>, or expand the section below for more details.</p><details><summary>More details</summary><p>Denoting by $\mathscr{P}(S)$ the set of probability distributions on a set
$S$, we define a Markov decision process $(\mathcal{X}, \mathcal{A}, \gamma,
P, r)$ as:</p><ul><li>A finite state space $\mathcal{X}$;</li><li>A finite action space $\mathcal{A}$;</li><li>A transition kernel $P : \mathcal{X} \times \mathcal{A}\rightarrow \mathscr{P}(\mathcal{X})$;</li><li>A reward function $r : \mathcal{X} \times\mathcal{A} \rightarrow \mathbb{R}$;</li><li>A discount factor $\gamma \in [0,1)$.</li></ul><p>For notational convenience we introduce the notation $P_x^a \in
\mathscr{P}(\mathcal{X})$ for the next-state distribution given state-action
pair $(x, a)$, and $r_x^a$ for the corresponding immediate reward.</p><p>Policies are mappings from states to distributions
over actions: $\pi \in \mathscr{P}(\mathcal{A})^\mathcal{X}$ and induce a
<em>value function</em> $V^{\pi}:\mathcal{X}\rightarrow\mathbb{R}$ defined via the recurrence:</p><p>$$V^{\pi}(x) := \mathbb{E}_{a\sim\pi(x)}\left[ r_x^a + \gamma\mathbb{E}_{x'\sim P_x^a} [V^{\pi}(x')]\right]$$</p><p>It can be shown that this recurrence uniquely defines $V^\pi$ through a contraction mapping argument.</p><p>The control problem is concerned with finding the optimal policy</p><p>$$ \pi^{*} = \arg\max_{\pi\in\mathscr{P}(\mathcal{A})^\mathcal{X}}V^{\pi} $$</p><p>It can be shown that while the optimisation problem above appears to have
multiple objectives (one for each coordinate of $V^\pi$, there is in fact a
policy $\pi^{*} \in \mathscr{P}(\mathcal{A})^\mathcal{X}$ that simultaneously
maximises all coordinates of $V^\pi$, and that this policy can be taken to be
deterministic; that is, for each $x \in \mathcal{X}$, $\pi(\cdot|x) \in
\mathscr{P}(\mathcal{A})$ attributes probability 1 to a single action. In
reinforcement learning in particular, we are often interested in finding, or
approximating, $\pi^{*}$ from direct interaction with the MDP in question via
sample trajectories, <em>without knowledge of $P$ or $r$</em> (and sometimes not
even $\mathcal{X}$).</p></details><h3 id=bisimulation-metrics>Bisimulation metrics</h3><p>Bisimulation metrics quantify the <em>behavioural distance</em> between two states in a Markov decision process.
I give a brief introduction to bisimulation metrics in <a href=/posts/research/rl/scalable/#bisimulation>this post</a>,
or you can read more details by expanding the section below.</p><details><summary>More details</summary><p>A <em>metric</em> $d$ on a set $X$ is a function $d:X\times X\rightarrow [0, \infty)$ respecting the following axioms for any $x, y, z \in X$:</p><ul><li><strong>Identity of indiscernibles:</strong> $d(x, y) = 0 \iff x = y$;</li><li><strong>Symmetry:</strong> $d(x, y) = d(y, x)$;</li><li><strong>Triangle inequality:</strong> $d(x, y) \leq d(x, z) + d(z, y)$.</li></ul><p>A <em>pseudometric</em> is similar, but the &ldquo;identity of indiscernibles&rdquo; axiom is weakened:</p><ul><li>$x = y \implies d(x, y) = 0$;</li><li>$d(x, y) = d(y, x)$;</li><li>$d(x, y) \leq d(x, z) + d(z, y)$.</li></ul><p>Note that the weakened first condition <em>does</em> allow one to have $d(x, y) = 0$ when $x\ne y$.</p><p>A <em>(pseudo)metric space</em> $(X, d)$ is defined as a set $X$ together with a (pseudo)metric $d$ defined on $X$.</p><p>Bisimulation is a fundamental notion of behavioural equivalence introduced by
<a href=https://dl.acm.org/doi/book/10.5555/534666>Park and Milner</a> in the early
1980s in the context of nondeterministic transition systems. The probabilistic
analogue was introduced by <a href=https://www.sciencedirect.com/science/article/pii/0890540191900306>Larsen and
Skou</a>. The
notion of an equivalence relation is not suitable to capture the extent to
which quantitative systems may resemble each other in behaviour. To provide a
quantitative notion, bisimulation metrics were introduced by
<a href=https://link.springer.com/chapter/10.1007/3-540-48320-9_19>[Desharnais et al., 1999]</a>
in the context of probabilistic transition systems without rewards. In
reinforcement learning the reward is an important ingredient, accordingly the
<em>bisimulation metric</em> for states of MDPs was introduced by
<a href=https://arxiv.org/abs/1207.4114>[Ferns et al., 2004]</a>.</p><p>Various notions of similarity between states in MDPs have been considered in
the RL literature, with applications in policy transfer, state aggregation, and
representation learning. The <em>bisimulation metric</em> is of
particular relevance for this paper, and defines state similarity in an MDP by
declaring two states $x,y \in \mathcal{X}$ to be close if their immediate
rewards are similar, and the transition dynamics at each state leads to next
states which are also judged to be similar.</p><p>Central to the definition of the bisimulation metric is the operator
$T_k : \mathcal{M}(\mathcal{X}) \rightarrow \mathcal{M}(\mathcal{X})$, defined
over $\mathcal{M}(\mathcal{X})$, the space of pseudometrics on $\mathcal{X}$.
We now turn to the definition of the operator itself, given by</p><p>$$T_k(d)(x, y) = \max_{a \in \mathcal{A}} [|r_x^a - r_y^a] + \gamma W_d(P^a_x, P^a_y)]$$</p><p>for each $d \in \mathcal{M}(\mathcal{X})$, and each
$x, y \in \mathcal{X}$. It can be verified that the function
$T_K(d) : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$
satisfies the properties of a pseudometric, so under this definition $T_K$ does
indeed map $\mathcal{M}(\mathcal{X})$ into itself.</p><p>The other central mathematical concept underpinning the operator $T_K$ is the Kantorovich distance $W_d$ (Commonly known as the Wasserstein distance) using base metric $d$. $W_d$ is formally a pseudometric over the set of probability distributions $\mathscr{P}(\mathcal{X})$, defined as the solution to an optimisation problem. The problem specifically is formulated as finding an optimal coupling between the two input probability distributions that minimises a notion of transport cost associated with $d$. Mathematically, for two probability distributions $\mu, \mu' \in \mathscr{P}(\mathcal{X})$, we have
\begin{align*}
W_d(\mu, \mu') = \min_{\substack{(Z, Z') \ Z \sim \mu, Z' \sim \nu'}} \mathbb{E}[d(Z, Z')] , .
\end{align*}
Note that the pair of random variables $(Z, Z')$ attaining the minimum in the
above expression will in general not be independent. That the minimum is
actually attained in the above example in the case of a finite set
$\mathcal{X}$ can be seen by expressing the optimisation problem as a linear
program. Minima are obtained in much more general settings too; see
<a href=https://cedricvillani.org/sites/dev/files/old_images/2012/08/preprint-1.pdf>Cedric Villani&rsquo;s book</a>
for more details.</p><p>The operator $T_K$ can be analysed in a similar way to standard operators in
dynamic programming for reinforcement learning. It can be shown that it is a
contraction mapping with respect to the $L^\infty$ metric over
$\mathcal{M}(\mathcal{X})$, and that $\mathcal{M}(\mathcal{X})$ is a complete
metric space with respect to the same metric.
Thus, by Banach&rsquo;s fixed point theorem, $T_K$ has a unique fixed point in
$\mathcal{M}(\mathcal{X})$, and repeated application of $T_K$ to any initial
pseudometric will converge to this fixed point.</p><p>Finally, Ferns et al. show that this metric bounds differences in the optimal
value function, hence its importance in RL:</p><p>$$|V^{*}(x) - V^{*}(y)| \leq d^\sim(x, y) \quad \forall x,y\in\mathcal{X}$$</p></details><h3 id=representation-learning-in-rl>Representation learning in RL</h3><p>In large-scale environments, RL agents must approximate value functions in a
more concise manner, by forming a <em>representation</em> of the environment.</p><details><summary>More details</summary><p>In large-scale environments, it is infeasible to express value functions
directly as vectors in $\mathbb{R}^{\mathcal{X} \times \mathcal{A}}$. Instead,
RL agents must approximate value functions in a more concise manner, by forming
a <em>representation</em> of the environment, that is, a feature embedding
$\phi: \mathcal{X} \rightarrow \mathbb{R}^M$, and predicting state-action values
linearly from these features. <em>Representation learning</em> is the problem of
finding a useful representation $\phi$. Increasingly, deep RL agents are
equipped with additional losses to aid representation learning. A common
approach is to require the agent to make additional predictions (so-called
<em>auxilliary tasks</em>) with its representation, typically with the aid of
extra network parameters, with the intuition that an agent is more likely to
learn useful features if it is required to solve many related tasks. We refer
to such methods as <em>implicit</em> representation shaping, since improved
representations are a side-effect of learning to solve auxiliary tasks.</p><p>Since bisimulation metrics capture additional information about the MDP in
addition to that summarised in value functions, bisimulation metrics are a
natural candidate for auxiliary tasks in deep reinforcement learning.
<a href=https://arxiv.org/abs/1906.02736>Gelada et al. [2019]</a>,
<a href=https://arxiv.org/abs/2006.10742>Zhang et al. [2021]</a>, and
<a href=https://arxiv.org/abs/2101.05265>Agarwal et al. [2021]</a>
introduce auxiliary tasks based on bisimulation
metrics, but require additional assumptions on the underlying MDP in order for
the metric to be learnt correctly (Lipschitz continuity, deterministic, and
Gaussian transitions, respectively). The success of these approaches provides
motivation in this paper to introduce a notion of state similarity applicable
to arbitrary MDPs, without further restriction. Further, we learn this state
similarity <em>explicitly</em>: that is, without the aid of any additional network
parameters.</p></details><h2 id=limitations-of-bisimulation-metrics>Limitations of bisimulation metrics</h2><p>Bisimulation metrics provide a strong notion of distance on the state space of
an MDP; however, they have been difficult to use at scale and compute online,
including the following reasons:</p><h3 id=computational-complexity>Computational complexity</h3><p>The metric can be computed via fixed-point
iteration since the operator $T_K$ is a contraction mapping. The map $T_K$ contracts at rate
$\gamma$ with respect to the $L^\infty$ norm on $\mathcal{M}$, and therefore
obtaining an $\varepsilon$-approximation of $d^\sim$ under this norm requires
$O(\log(\varepsilon) / \log(\gamma))$ applications of $T_K$ to
an initial pseudometric $d_0$. The cost of each application of $T_K$
is dominated by the computation of $|\mathcal{X}|^2|\mathcal{A}|$ $W_d$
distances for distributions over $\mathcal{X}$, each costing
$\tilde{O}(|\mathcal{X}|^{2.5})$ in theory, and $\tilde{O}(|\mathcal{X}|^3)$ in
practice. Thus, the overall practical cost is
$\tilde{O}(|\mathcal{X}|^{5}|\mathcal{A}|\log(\varepsilon) / \log(\gamma))$.</p><p>This property expresses the intrinsic computational difficulty of computing this
metric.</p><h3 id=bias-under-sampled-transitions>Bias under sampled transitions.</h3><p>Computing $T_K$ requires access to the transition probability distributions
$P_x^a$ for each $(x, a) \in \mathcal{X} \times \mathcal{A}$ which, as
mentioned <a href=#background>above</a>, are typically not available; instead,
stochastic approximations to the operator of interest are employed. Whilst
there has been work in studying online, sample-based approximate computation of
the bisimulation metric, these methods are generally biased, in contrast to
sample-based estimation of standard RL operators.</p><p>This property illustrates the problems associated with attempting to move from
operator-based computation to online, sampled-based computation of the metric
(for example, when the environment dynamics are unknown).</p><h3 id=lack-of-connection-to-non-optimal-policies>Lack of connection to non-optimal policies</h3><p>One of the principal behavioural characterisations of the bisimulation metric
$d^\sim$ is the upper bound shown <a href=#bisimulation-metrics>above</a>. However, in
general we do not have</p><p>$$|V^\pi(x) - V^\pi (y)| \leq d^\sim(x, y)$$</p><p>for arbitrary policies $\pi \in \Pi$; a simple example is illustrated below:</p><img src=/posts/research/rl/mico/counterExample.png alt=counterExample width=30% class=center><p>In this MDP, $d^\sim(x, y) = (1-\gamma)^{-1}$, but for the policy $\pi(b|x)=1, \pi(a|y) = 1$, we have $|V^\pi(x) - V^\pi(y)| = k(1-\gamma)^{-1}$.</p><p>More generally, notions of state similarity that the bisimulation metric encodes may not be closely related to behavioural similarity under the policy $\pi$. Thus, learning about $d^\sim$ may not in itself be useful for large-scale reinforcement learning agents.</p><p>This property shows that even if the metric is computable exactly, the
information it yields about the MDP may not be practically useful. Although
$\pi$-bisimulation (introduced by me <a href=/posts/research/rl/scalable/>here</a>) and extended by
<a href=https://arxiv.org/abs/2006.10742>Zhang et al. [2021]</a>) addresses this property, their
practical algorithms are limited to MDPs with deterministic transitions or
MDPs with Gaussian transition kernels, respectively.</p><h2 id=the-mico-distance>The MICo distance</h2><p>We now present a new notion of distance for state similarity, which we refer to
as <em>MICo</em> (<strong>M</strong>_atching under _<strong>I</strong>_ndependent _<strong>Co</strong><em>uplings</em>), designed to
overcome the drawbacks described above. We make some modifications to
$T_K$ to deal with the previously mentioned shortcomings, detailed below.</p><p>In order to deal with the prohibitive cost of computing the Kantorovich
distance, which optimizes over all coupling of the distributions $P_x^a$ and
$P_y^a$, we use the independent coupling.</p><p>To deal with lack of connection to non-optimal policies, we consider an
on-policy variant of the metric, pertaining to a chosen policy $\pi \in
\mathscr{P}(\mathcal{A})^\mathcal{X}$. This leads us to the following
definition.</p><blockquote><p><strong>Definition</strong></p><blockquote><p>Given a policy $\pi \in\mathscr{P}(\mathcal{A})^\mathcal{X}$,
the MICo update operator, $T^\pi_M : \mathbb{R}^{\mathcal{X}\times\mathcal{X}} \rightarrow \mathbb{R}^{\mathcal{X}\times\mathcal{X}}$, is defined by</p><p>$$(T^\pi_M U)(x, y) = |r^\pi_x - r^\pi_y| + \gamma \mathbb{E}_{\begin{subarray}{l}x'\sim P^{\pi}_x \ y'\sim P^{\pi}_y\end{subarray}} \left[ U(x', y') \right]$$</p><p>for all functions $U:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$, with $r^\pi_x = \sum_{a \in \mathcal{A}} \pi(a|x) r_x^a$ and $P^{\pi}_x = \sum_{a\in\mathcal{A}}\pi(a|x)P_x^a(\cdot)$ for all $x \in \mathcal{X}$.</p></blockquote></blockquote><p>As with the bisimulation operator, this can be thought of as encoding desired
properties of a notion of similarity between states in a self-referential
manner; the similarity of two states $x, y \in \mathcal{X}$ should be determined
by the similarity of the rewards and the similarity of the states they lead to.</p><blockquote><p><strong>Proposition</strong></p><blockquote><p>The MICo operator $T^\pi_M$ is a contraction mapping on $\mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ with respect to the $L^\infty$ norm.</p></blockquote></blockquote><details><summary>Proof</summary><p>Let $U, U' \in \mathbb{R}^{\mathcal{X}\times\mathcal{X}}$. Then note that</p><p>$$|(T^\pi U)(x, y) - (T^\pi U')(x, y)| = \left|\gamma\sum_{x', y'}\pi(a|x)\pi(b|y)P_x^a(x')P_y^b(y') (U - U')(x', y') \right| \leq \gamma |U - U'|_\infty$$</p><p>for any $x,y \in \mathcal{X}$, as required.</p><div align=right>$\square$</div></details><p>The following corollary now follows immediately from Banach&rsquo;s fixed-point theorem and the completeness of $\mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ under the $L^\infty$ norm.</p><blockquote><p><strong>Corollary</strong></p><blockquote><p>The MICo operator $T^\pi_M$ has a unique fixed point $U^\pi \in
\mathbb{R}^{\mathcal{X}\times\mathcal{X}}$, and repeated application of
$T^\pi_M$ to any initial function $U \in
\mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ converges to $U^\pi$.</p></blockquote></blockquote><p>Having defined a new operator, and shown that it has a corresponding fixed-point, there are two questions to address:</p><ul><li>Does this new notion of distance overcome the drawbacks of the bisimulation metric described above?</li><li>What does this new object tell us about the underlying MDP?</li></ul><h3 id=addressing-the-drawbacks-of-the-bisimulation-metric>Addressing the drawbacks of the bisimulation metric</h3><p>In this section, we provide a series of results that show that the newly-defined notion of distance addressess each of these shortcomings presented previously. The proofs of these results rely on the following lemma, connecting the MICo operator to a lifted MDP.</p><blockquote><p><strong>Lemma (Lifted MDP)</strong></p><blockquote><p>The MICo operator $T^\pi_M$ is the Bellman evaluation operator for an auxiliary MDP.</p></blockquote></blockquote><details><summary>Proof</summary>
Given the MDP specified by the tuple $(\mathcal{X}, \mathcal{A}, P, R)$, we construct an auxiliary MDP $(\widetilde{\mathcal{X}},\widetilde{\mathcal{A}},
\widetilde{P}, \widetilde{R})$ defined by:
* State space $\widetilde{\mathcal{X}} = \mathcal{X}^2$
* Action space $\widetilde{\mathcal{A}} = \mathcal{A}^2$
* Transition dynamics given by $\widetilde{P}\_{(u, v)}^{(a, b)}((x,y)) = P\_u^a(x)P\_v^b(y)$ for all $(x,y), (u,v) \in \mathcal{X}^2$, $a,b \in \mathcal{A}$
* Action-independent rewards $\widetilde{R}\_{(x,y)} = |r^\pi\_x - r^\pi\_y|$ for all $x, y \in \mathcal{X}$.<p>The Bellman evaluation operator $\widetilde{T}^{\tilde{\pi}}$ for this auxiliary MDP at discount rate $\gamma$ under the policy $\tilde{\pi}(a,b|x,y) = \pi(a|x) \pi(b|y)$ is given by:
$$ (\widetilde{T}^{\tilde{\pi}}U)(x,y) = \widetilde{R}_{(x,y)} + \gamma \sum_{(x^\prime, y^\prime) \in \mathcal{X}^2} \widetilde{P}_{(x, y)}^{(a, b)}((x^\prime, y^\prime)) \tilde{\pi}(a,b|x,y) U(x^\prime, y^\prime)$$
$$ \qquad = |r^\pi_x - r^\pi_y| + \gamma \sum_{(x^\prime, y^\prime) \in \mathcal{X}^2} P^\pi_x(x^\prime)P_y^\pi(y^\prime) U(x^\prime, y^\prime)$$
$$ = (T^\pi_MU)(x, y) \qquad\qquad \qquad\qquad \qquad\quad$$</p><p>for all $U \in \mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ and $(x, y) \in \mathcal{X} \times\mathcal{X}$, as required.</p><div align=right>$\square$</div></details><p>Equipped with the above lemma, we can address the three limitations listed <a href=#limitations-of-bisimulation-metrics>above</a>:</p><h4 id=computational-complexity-1>Computational complexity</h4><blockquote><p><strong>Proposition</strong></p><blockquote><p>The computational complexity of computing an $\varepsilon$-approximation in $L^\infty$ to the MICo metric is $O(|\mathcal{X}|^4 \log(\varepsilon) / \log(\gamma))$.</p></blockquote></blockquote><details><summary>Proof</summary><p>Since the operator $T^\pi_M$ is a $\gamma$-contraction under $L^\infty$, we require $\mathcal{O}(\log(\varepsilon) / \log(\gamma))$ applications of the operator to obtain an $\varepsilon$-approximation in $L^\infty$. Each iteration of value iteration updates $|\mathcal{X}|^2$ table entries, and the cost of each update is $\mathcal{O}(|\mathcal{X}|^2)$, leading to an overall cost of $O(|\mathcal{X}|^4\log(\varepsilon) / \log(\gamma))$.</p><div align=right>$\square$</div></details><p>In contrast to the bisimulation metric, this represents a computational saving
of $O(|\mathcal{X}|)$, which arises from the lack of a need to
solve optimal transport problems over the state space in computing the MICo
distance. There is a further saving of $\mathcal{O}(|\mathcal{A}|)$ that arises
since MICo focuses on an individual policy $\pi$, and so does not require the
max over actions in the bisimulation operator definition.</p><h4 id=online-approximation>Online approximation</h4><blockquote><p><strong>Proposition</strong></p><blockquote><p>Suppose rewards depend only on state, and consider the sequence of estimates $(U_t)_{t \geq 0}$, with $U_0$ initialised arbitrarily, and $U_{t+1}$ updated from $U_t$ via a pair of transitions $(x_t, a_t, r_t, x'_t)$, $(y_t, b_t, \tilde{r}_t, y'_t)$ as:
$$U_{t+1}(x, y) \leftarrow (1-\epsilon_t(x, y))U_t(x, y) + \epsilon_t(x, y) ( |r - \tilde{r}| + \gamma U_{t}(x', y') )$$
Suppose all state-pairs tuples are updated infinitely often, and stepsizes for these updates satisfy the Robbins-Monro conditions. Then $U_t \rightarrow U^\pi$ almost surely.</p></blockquote></blockquote><details><summary>Proof</summary><p>Due to the interpretation of the MICo operator $T^\pi_M$ as the Bellman evaluation operator in an auxiliary MDP, algorithms and associated proofs of correctness for computing the MICo distance online can be straightforwardly derived from standard online algorithms for policy evaluation.
Under the assumptions of the proposition, the update described is exactly a TD(0) update in the lifted MDP described above. We can therefore appeal to Proposition~4.5 of <a href=http://athenasc.com/ndpbook.html>Bertsekas and Tsitsiklis [1996]</a> to obtain the result.
Note that the wide range of online policy evaluation methods incorporating off-policy corrections and multi-step returns, as well as techniques for applying such methods at scale, may also be used.</p><div align=right>$\square$</div></details><h4 id=relationship-to-underlying-policy>Relationship to underlying policy</h4><blockquote><p><strong>Proposition</strong></p><blockquote><p>For any policy $\pi \in \mathscr{P}(\mathcal{A})^{\mathcal{X}}$ and states $x,y \in \mathcal{X}$, we have $|V^\pi(x) - V^\pi(y)| \leq U^\pi(x, y)$.</p></blockquote></blockquote><details><summary>Proof</summary><p>We apply a coinductive argument <a href=https://www.cs.cornell.edu/~kozen/Papers/coinduction.pdf>[Kozen, 2007]</a> to show that if
\begin{align}\label{eq:pf1}
|V^\pi(x) - V^\pi(y)| \leq U(x, y) \ \text{for all } x, y \in \mathcal{X} ,
\end{align}
for some $U \in \mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ symmetric in its two arguments, then we also have
$$|V^\pi(x) - V^\pi(y)| \leq (T^\pi_M U)(x, y) \ \text{for all } x, y \in \mathcal{X}$$</p><p>Since the hypothesis holds for the constant function $U(x,y) = 2 R_\text{max}/(1-\gamma)$, and $T^\pi_M$ contracts around $U^\pi$, the conclusion then follows. Therefore, suppose the coinductive hypothesis holds. Then we have
$$V^\pi(x) - V^\pi(y) = r^\pi_xx - r^\pi_y + \gamma \sum_{x' \in \mathcal{X}} P^\pi_x(x') V(x') - \gamma \sum_{y' \in \mathcal{X}} P^\pi_y(y') V(y') $$
$$\leq |r^\pi_x - r^\pi_y| + \gamma \sum_{x', y' \in \mathcal{X}} P^\pi_x(x')P^\pi_y(y') (V^\pi(x') - V^\pi(y')) $$
$$\leq |r^\pi_x - r^\pi_y| + \gamma \sum_{x', y' \in \mathcal{X}} P^\pi_x(x')P^\pi_y(y') U(x', y') $$
$$= (T^\pi_M U)(x, y) $$</p><p>By symmetry, $V^\pi(y) - V^\pi(x) \leq (T^\pi_M U)(x, y)$, as required.</p><div align=right>$\square$</div></details><h3 id=diffuse-metrics>Diffuse metrics</h3><p>To characterize the nature of the fixed point $U^\pi$, we introduce a novel notion of distance which we name <em>diffuse metrics</em>, which we define below.</p><blockquote><p><strong>Definition (Diffuse metrics)</strong></p><blockquote><p>Given a set $\mathcal{X}$, a function $d:\mathcal{X}\times \mathcal{X} \to \mathbb{R}$ is a diffuse metric if the following axioms hold:</p><ul><li>$d(x,y)\geq 0$ for any $x,y\in \mathcal{X},$</li><li>$d(x,y)=d(y,x)$ for any $x,y\in \mathcal{X},$</li><li>$d(x,y)\leq d(x,z)+d(y,z)$ $\forall x,y,z\in \mathcal{X}.$</li></ul></blockquote></blockquote><p>These differ from the standard metric axioms in the first point: we no longer
require that a point has zero self-distance, and two distinct points
may have zero distance. Notions of this kind are increasingly common in machine
learning as researchers develop more computationally tractable versions of
distances, as with entropy-regularised optimal transport distances
<a href=https://arxiv.org/abs/1306.0895>[Cuturi, 2013]</a>, which also do not satisfy the axiom of zero
self-distance.</p><details><summary>Extra details (including an interactive widget!)</summary><p>An example of a diffuse metric is the Łukaszyk–Karmowski distance
<a href=https://link.springer.com/article/10.1007/s00466-003-0532-2>[Łukaszyk, 2003]</a>,
which is used in the MICo metric as the operator between the
next-state distributions. Given a diffuse metric space $(\mathcal{X}, \rho)$, the Łukaszyk–Karmowski distance $d^{\rho}_{\text{LK}}$ is a diffuse metric on probability measures on $\mathcal{X}$ given by</p><p>$$d^\rho_{\text{LK}}(\mu,\nu)=\mathbb{E}_{x\sim \mu, y\sim \nu}[\rho(x,y)]$$</p><p>This example demonstrates the origin of the name <em>diffuse</em> metrics; the
non-zero self distances arises from a point being spread across a probability
distribution.</p><p>The following interactive example can help illustrate this notion of <em>diffusion</em>.
Given a normal distribution $\mu = \mathcal{N}(0.0, \sigma)$ (displayed with the blue line)
we can draw <em>numPoints</em> samples (pink dots) and estimate the self-distance
$d^\rho_{\text{LK}}(\mu, \mu)$, where $\rho(x, y) := |x - y|$. You can vary <em>stdDev</em> to change
the $\sigma$ parameter of $\mu$ and observe how the distance changes; in particular, when
$\sigma = 0$ (i.e. $\mu$ is a Dirac-delta distribution) we have $0$ self-distance.</p><code>numPoints: <input id=numPoints placeholder=100 value=100 onchange=generatePlot() type=text onkeyup="this.value=limits(this.value,2,1000)">
stdDev: <input id=stdDev placeholder=1.0 value=1.0 onchange=generatePlot() type=text onkeyup="this.value=limits(this.value,0.0,1000.0)">
<button onclick=generatePlot()>Regenerate samples</button></code><div id=graph></div><script>window.PlotlyConfig={MathJaxConfig:'local'}</script><script src=https://cdn.plot.ly/plotly-latest.min.js></script><script src=https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.5></script><script src=/posts/research/rl/mico/script.js></script><p>The notion of a distance function having non-zero self distance was first
introduced by <a href=https://www.dcs.warwick.ac.uk/pmetric/Ma94.pdf>[Matthews, 1994]</a> who called it a <em>partial metric</em>. We
define it below:</p><blockquote><p><strong>Definition (Partial metric)</strong></p><blockquote><p>Given a set $\mathcal{X}$, a function $d:\mathcal{X}\times \mathcal{X} \to \mathbb{R}$ is a partial metric if</p><ul><li>$x=y \iff d(x,x)=d(y,y)=d(x,y)$ for any $x,y\in \mathcal{X},$</li><li>$d(x,x)\leq d(y,x)$ for any $x,y\in \mathcal{X},$</li><li>$d(x,y)= d(y,x)$ for any $x,y\in \mathcal{X},$</li><li>$d(x,y)\leq d(x,z)+d(y,z)-d(z, z)$ $\forall x,y,z\in \mathcal{X}.$</li></ul></blockquote></blockquote><p>This definition was introduced to recover a proper metric from the distance
function: that is, given a partial metric $d$, one is guaranteed that
$\tilde{d}(x,y)=d(x,y)-\frac{1}{2}\left(d(x,x)+d(y,y)\right)$ is a proper
metric.</p><p>The above definition is still too stringent for the Łukaszyk–Karmowski distance (and hence MICo distance), since it fails axiom 4 (the modified triangle inequality) as shown in the following counterexample.</p><blockquote><p><strong>Example</strong></p><blockquote><p>The Łukaszyk–Karmowski distance does not satisfy the modified triangle inequality: let $\mathcal{X}$ be $[0,1]$, and $\rho$ be the Euclidean distance $|\cdot|$. Let $\mu$,$\nu$ be Dirac measures concentrated at 0 and 1, and let $\eta$ be $\frac{1}{2}(\delta_0+\delta_1)$. Then one can calculate that $d_{LK}(\rho)(\mu,\nu)=1$, while $d_{LK}(\rho)(\mu,\eta)+d_{LK}(\rho)(\nu,\eta)-d_{LK}(\rho)(\eta,\eta)=1/2$, breaking the inequality.</p></blockquote></blockquote><p>In terms of the Łukaszyk–Karmowski distance, the MICo distance can be written as the fixed point
$$ U^\pi(x,y)=|r^\pi_x-r^\pi_y|+d_{\text{LK}}(U^\pi) (P^\pi_x,P^\pi_y)$$</p><p>This characterisation leads to the following result.</p></details><blockquote><p><strong>Proposition</strong></p><blockquote><p>The MICo distance is a diffuse metric.</p></blockquote></blockquote><details><summary>Proof</summary>
Non-negativity and symmetry of $U^\pi$ are clear, so it remains to check the triangle inequality. To do this, we define a sequence of iterates $(U\_k)\_{k \geq 0}$ in $ \mathbb{R}^{\mathcal{X}\times\mathcal{X}}$ by $U\_0(x, y) = 0$ for all $x, y \in \mathcal{X}$, and $ U\_{k+1} = T^\pi\_M U\_k$ for each $k \geq 0$. Recall that by \autoref{corr:mico-fp} that $U\_k \rightarrow U^\pi$. We will show that each $U\_k$ satisfies the triangle inequality by induction. By taking limits on either side of the inequality, we will then recover that $U^\pi$ itself satisfies the triangle inequality.<p>The base case of the inductive argument is clear from the choice of $U_0$. For the inductive step, assume that for some $k \geq 0$, $U_k(x,y) \leq U_k(x, z) + U_k(z, y)$ for all $x, y, z \in \mathcal{X}$. Now for any $x, y, z \in \mathcal{X}$, we have
$$U_{k+1}(x, y) = |r^\pi_x - r^\pi_y| + \gamma \mathbb{E}_{X' \sim P^\pi_x, Y' \sim P^\pi_y}[U_k(X', Y')]\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad$$
$$\leq |r^\pi_x - r^\pi_z| + |r^\pi_z - r^\pi_y| + \gamma \mathbb{E}_{X' \sim P^\pi_x, Y' \sim P^\pi_y, Z' \sim P^\pi_z}[U_k(X', Z') + U_k(Z', Y')]$$
$$= U_{k+1}(x, z) + U_{k+1}(z, y)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$$</p><p>as required.</p><div align=right>$\square$</div></details><p>Note that a state $x \in \mathcal{X}$ has zero self-distance iff the Markov chain induced by $\pi$ initialised at $x$ is deterministic. Indeed, the
magnitude of a state&rsquo;s self-distance is indicative of the amount of &ldquo;dispersion&rdquo;
in the distribution. Hence, in general, we have $U^\pi(x, x) > 0$, and $U^\pi(x, x) \not= U^\pi(y, y)$ for distinct states $x, y \in \mathcal{X}$.</p><h2 id=the-mico-loss>The MICo loss</h2><p>The impetus of our work is the development of principled mechanisms for directly shaping the representations used by RL agents so as to improve their learning. In this section we present a novel loss based on the MICo update operator $T^\pi_M$ that can be incorporated into any value-based agent. Given the fact that MICo is a diffuse metric that can admit non-zero self-distances, special care needs to be taken in how these distances are learnt; indeed, traditional mechanisms for measuring distances between representations (e.g. Euclidean and cosine distances) are geometrically-based and enforce zero self-distances.</p><p>We assume a value-based agent learning an estimate $Q_{\xi,\omega}$ defined by the composition of two function approximators $\psi$ and $\phi$ with parameters $\xi$ and $\omega$, respectively: $Q_{\xi, \omega}(x, \cdot) = \psi_{\xi}(\phi_{\omega}(x))$. We will refer to $\phi_{\omega}(x)$ as the <em>representation</em> of state $x$ and aim to make distances between representations match the MICo distance; we refer to $\psi_{\xi}$ as the <em>value approximator</em>.
We define the parameterized representation distance, $U_{\omega}$, as an approximant to $U^{\pi}$:
$$U^{\pi}(x, y) \approx U_{\omega}(x, y) := \frac{| \phi_{\omega}(x) |_2 + | \phi_{\omega}(y) |_2 }{2} + \beta \theta(\phi_{\omega}(x), \phi_{\omega}(y))$$
where $\theta(\phi_\omega(x), \phi_\omega(y))$ is the angle between vectors $\phi_\omega(x)$ and $\phi_\omega(y)$ and $\beta$ is a scalar.
Our learning target is then
$$T^U_{\bar{\omega}}(r_x, x', r_y, y') = |r_x - r_y| + \gamma U_{\bar{\omega}}(x', y')$$
where $\bar{\omega}$ is a separate copy of the network parameters that are synchronised with $\omega$ at infrequent intervals. This is a common practice that was introduced by <a href=https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning>Mnih et al., [2015]</a> (and in fact, we use the same update schedule they propose). The loss for this learning target is
$$\mathcal{L}_{\text{MICo}}(\omega) = \mathbb{E}_{\langle x, r_x, x'\rangle , \langle y, r_y, y'\rangle}\left[ \left(T^U_{\bar{\omega}}(r_x, x', r_y, y') - U_{\omega}(x, y)\right)^2 \right]$$</p><p>where $\langle x, r_x, x'\rangle$ and $\langle y, r_y, y'\rangle$ are pairs of transitions sampled from the agent&rsquo;s replay buffer.
We can combine $\mathcal{L}_{\text{MICo}}$ with the temporal-difference loss $\mathcal{L}_{\text{TD}}$ of any value-based agent as $(1-\alpha)\mathcal{L}_{\text{TD}} + \alpha\mathcal{L}_{\text{MICo}}$, where $\alpha \in (0,1)$. Each sampled mini-batch is used for both MICo and TD losses. The figure below illustrates the network architecture used for learning:</p><img src=/posts/research/rl/mico/networkArchitecture.png alt="Network Architecture" width=50% class=center><p>Although the loss $\mathcal{L}_{\text{MICo}}$ is designed to learn the MICo diffuse metric $U^\pi$, the values of the metric itself are parametrised through $U_\omega$ defined above, which is constituted by several distinct terms. This appears to leave a question as to how the representations $\phi_\omega(x)$ and $\phi_\omega(y)$, as Euclidean vectors, are related to one another when the MICo loss is minimised. Careful inspection of the form of $U_\omega(x, y)$ shows that the (scaled) angular distance between $\phi_\omega(x)$ and $\phi_\omega(y)$ can be recovered from $U_\omega$ by subtracting the learnt approximations to the self-distances $U^\pi(x, x)$ and $U^\pi(y,y)$, as the figure below illustrates:</p><img src=/posts/research/rl/mico/projection.png alt="MICo projection" width=50% class=center><p>We therefore define the reduced MICo distance $\Pi U^\pi$, which encodes the distances enforced between the representation vectors $\phi_\omega(x)$ and $\phi_\omega(y)$, by:
$$\beta \theta(\phi_{\omega}(x) ,\phi_{\omega}(y)) \approx \Pi U^{\pi}(x, y) = U^{\pi}(x, y) - \frac{1}{2}U^{\pi}(x, x) - \frac{1}{2}U^{\pi}(y, y)$$</p><p>In the following sections we investigate the following two questions: {\bf (1)} How informative of $V^{\pi}$ is $\Pi U^{\pi}$?; and {\bf (2)} How useful are the features encountered by $\Pi U^{\pi}$ for policy evaluation? We conduct these investigations on tabular environments where we can compute the metrics exactly, which helps clarify the behaviour of our loss when combined with deep networks.</p><h3 id=value-bound-gaps>Value bound gaps</h3><p>Although we have $|V^{\pi}(x) - V^{\pi}(y)| \leq U^{\pi}(x, y)$, we do not, in general, have the same upper bound for $\Pi U^{\pi}(x, y)$ as demonstrated by the following result.</p><blockquote><p><strong>Lemma</strong></p><blockquote><p>There exists an MDP with two states $x$, $y$, and a policy $\pi\in\Pi$ where $|V^{\pi}(x) - V^{\pi}(y)| > \Pi U^{\pi}(x, y)$.</p></blockquote></blockquote><details><summary>Proof</summary><p>Consider a single-action MDP with two states ($x$ and $y$) where $y$ is absorbing, $x$ transitions with equal probability to $x$ and $y$, and a reward of $1$ is received only upon taking an action from state $x$.</p><p>There is only one policy for this MDP which yields the value function $V(x) \approx 1.8$ and $V(y) = 0$.</p><p>The MICo distance gives $U(x, x) \approx 1.06$, $U(x, y) \approx 1.82$, and $U(y, y) = 0$, while the reduced MICo distance yields $\Pi U(x, x) = \Pi U(y, y) = 0$, and
$$\Pi U(x, y) \approx 1.29 &lt; |V(x) - V(y)| = 1.8$$</p><div align=right>$\square$</div></details><p>Despite this negative result, it is worth evaluating how often <em>in practice</em> this inequality is violated and by how much, as this directly impacts the utility of this distance for learning representations. To do so in an unbiased manner we make use of Garnet MDPs, which are a class of randomly generated MDPs.</p><details><summary>Details of Garnet MDPs</summary>
Given a specified number of states $n\_{\mathcal{X}}$ and the number of actions $n\_{\mathcal{A}}$, $\text{Garnet}(n\_{\mathcal{X}}, n\_{\mathcal{A}})$ is generated as follows:<ol><li>The branching factor $b_{x, a}$ of each transition $P_x^a$ is sampled uniformly from $[1:n_{\mathcal{X}}]$.</li><li>$b_{x, a}$ states are picked uniformly randomly from $\mathcal{X}$ and assigned a random value in $[0, 1]$; these values are then normalized to produce a proper distribution $P_x^a$.</li><li>Each $r_x^a$ is sampled uniformly in $[0, 1]$.</li></ol></details><p>For each $\text{Garnet}(n_{\mathcal{X}}, n_{\mathcal{A}})$ we sample 100 stochastic policies ${\pi_i}$ and compute the average gap: $\frac{1}{100 |\mathcal{X}|^2}\sum_i \sum_{x, y} d(x, y) - |V^{\pi_i}(x) - V^{\pi_i}(y)|$, where $d$ stands for any of the considered metrics.
Note we are measuring the <em>signed</em> difference, as we are interested in the frequency with which the upper-bound is violated.
As seen in the figure below, our metric <em>does</em> on average provide an upper bound on the difference in values that is also tighter bound than those provided by $U^{\pi}$ and $\pi$-bisimulation. This suggests that the resulting representations remain informative of value similarities, despite the reduction $\Pi$.</p><img src=/posts/research/rl/mico/valueGaps.png alt="Value gaps" width=50% class=center><h3 id=state-features>State features</h3><p>In order to investigate the usefuleness of the representations produced by $\Pi U^{\pi}$, we construct state features directly by using the computed distances to project the states into a lower-dimensional space with the <a href=https://arxiv.org/abs/1802.03426>UMAP</a> dimensionality reduction algorithm (Note that since UMAP expects a metric, it is ill-defined with the diffuse metric $U^{\pi}$.). We then apply linear regression of the true value function $V^{\pi}$ against the features to compute $\hat{V^{\pi}}$ and measure the average error across the state space. As baselines we compare against random features (RF), Proto Value Functions (PVF) <a href=https://www.jmlr.org/papers/volume8/mahadevan07a/mahadevan07a.pdf>[Mahadevan, 2007]</a>, and the features produced by <a href=/posts/research/rl/scalable/>$\pi$-bisimulation</a>. We present our results on three domains in in the figure below, the classic four-rooms GridWorld (left), the mirrored rooms introduced in <a href=/posts/research/rl/scalable/>my paper</a>, and the grid task introduced by <a href=http://www.gatsby.ucl.ac.uk/~dayan/papers/d93b.pdf>Dayan, [1993]</a>:</p><img src=/posts/research/rl/mico/stateFeatures.png alt="State features" width=80% class=center><p>Despite the independent couplings, $\Pi U^{\pi}$ performs on par with $\pi$-bisimulation, which optimizes over all transition probability couplings, suggesting that $\Pi U^{\pi}$ yields good representations.</p><h2 id=empirical-evaluation>Empirical evaluation</h2><p>Having developed a greater understanding of the properties inherent to the representations produced by the MICo loss, we evaluate it on the <a href=https://jair.org/index.php/jair/article/view/10819>Arcade Learning Environment</a>.
The code necessary to run these experiments is <a href=https://github.com/google-research/google-research/tree/master/mico>available on GitHub</a>.
We will first describe the regular network and training setup for these agents so as to facilitate the description of our loss.</p><details><summary>Baseline network and loss description</summary><p>The networks used by Dopamine for the ALE consist of 3 convolutional layers followed by two fully-connected layers (the output of the networks depends on the agent). We denote the output of the convolutional layers by $\phi_{\omega}$ with parameters $\omega$, and the remaining fully connected layers by $\psi_{\xi}$ with parameters $\xi.$ Thus, given an input state $x$ (e.g. a stack of 4 Atari frames), the output of the network is $Q_{\xi,\omega}(x, \cdot) = \psi_{\xi}(\phi_{\omega}(x))$. Two copies of this network are maintained: an <em>online</em> network and a <em>target</em> network; we will denote the parameters of the target network by $\bar{\xi}$ and $\bar{\omega}$. During learning, the parameters of the online network are updated every 4 environment steps, while the target network parameters are synced with the online network parameters every 8000 environment steps.
We refer to the loss used by the various agents considered as $\mathcal{L}_{\text{TD}}$; for example, for DQN this would be:
$$\mathcal{L}_{\text{TD}}(\xi, \omega) := \mathbb{E}_{(x, a, r, x')\sim\mathcal{D}}\left[\rho\left(r + \gamma\max_{a'\in\mathcal{A}}Q_{\bar{\xi},\bar{\omega}}(x', a') - Q_{\xi, \omega}(x, a) \right) \right]$$
where $\mathcal{D}$ is a replay buffer with a capacity of 1M transitions, and $\rho$ is the Huber loss.</p></details><h3 id=mico-loss-description>MICo loss description</h3><p>We will be applying the MICo loss to $\phi_{\omega}(x)$. As described in \autoref{sec:loss}, we express the distance between two states as:
$$U_{\omega}(x, y) = \frac{| \phi_{\omega}(x) |_2 + | \phi_{\bar{\omega}}(y) |_2 }{2} + \beta \theta(\phi_{\omega}(x), \phi_{\bar{\omega}}(y))$$</p><p>where $\theta(\phi_\omega(x), \phi_{\bar{\omega}}(y))$ is the angle between vectors $\phi_\omega(x)$ and $\phi_{\bar{\omega}}(y)$ and $\beta$ is a scalar.
Note that we are using the target network for the $y$ representations; this was done for learning stability. We used $\beta=0.1$ for the results in the main paper, but present some results with different values of $\beta$ below.</p><details><summary>Details on angular distance computation</summary><p>In order to get a numerically stable operation, we implement the angular distance between representations $\phi_\omega(x)$ and $\phi_\omega(y)$ according to the calculations
$$\text{CS}(\phi_\omega(x), \phi_\omega(y)) = \frac{\langle \phi_\omega(x), \phi_\omega(y)\rangle }{|\phi_\omega(x)||\phi_\omega(y)|}$$
$$\theta(\phi_\omega(x), \phi_\omega(y)) = \arctan2\left(\sqrt{1 - \text{CS}(\phi_\omega(x), \phi_\omega(y))^2}, \text{CS}(\phi_\omega(x), \phi_\omega(y))\right)$$</p></details><p>Based on the <a href=#the-mico-distance>MICo update operator</a>, our learning target is then (note the target network is used for both representations here):
$$T^U_{\bar{\omega}}(r_x, x', r_y, y') = |r_x - r_y| + \gamma U_{\bar{\omega}}(x', y')$$
and the loss is
$$\mathcal{L}_{\text{MICo}}(\omega) = \mathbb{E}_{\langle x, r_x, x'\rangle, \langle y, r_y, y'\rangle\sim\mathcal{D}}\left[ \left(T^U_{\bar{\omega}}(r_x, x', r_y, y') - U_{\omega}(x, y)\right)^2 \right]$$</p><p>We found it important to use the Huber loss to minimize $\mathcal{L}_{\text{MICo}}$ as this emphasizes greater accuracy for smaller distances as oppoosed to larger distances. We experimented using the MSE loss but found that larger distances tended to overwhelm the optimization process, thereby degrading performance.</p><p>As mentioned <a href=#the-mico-loss>above</a>, we use the same mini-batch sampled for $\mathcal{L}_{\text{TD}}$ for computing $\mathcal{L}_{\text{MICo}}$. Specifically, we follow the method <a href=/posts/research/rl/scalable/>I introduced</a> for constructing new matrices that allow us to compute the distances between all pairs of sampled states (see code for details on matrix operations).</p><p>Our combined loss is then:
$$\mathcal{L}_{\alpha}(\xi, \omega) = (1-\alpha)\mathcal{L}_{\text{TD}}(\xi, \omega) + \alpha\mathcal{L}_{\text{MICo}}(\omega)$$</p><h3 id=results>Results</h3><h4 id=ale-experiments>ALE experiments</h4><p>We added the MICo loss to all the JAX agents provided in the <a href=https://github.com/google/dopamine>Dopamine library</a>.
For all experiments we used the hyperparameter settings provided with Dopamine. We found that a value of $\alpha=0.5$ worked well with quantile-based agents (QR-DQN, IQN, and M-IQN), while a value of $\alpha=0.01$ worked well with DQN and Rainbow. We hypothesise that the difference in scale of the quantile, categorical, and non-distributional loss functions concerned leads to these distinct values of $\alpha$ performing well.
We found it important to use the Huber loss to minimize $\mathcal{L}_{\text{MICo}}$ as this emphasizes greater accuracy for smaller distances as oppoosed to larger distances. We experimented using the MSE loss but found that larger distances tended to overwhelm the optimization process, thereby degrading performance. We evaluated on all 60 Atari 2600 games over 5 seeds.</p><p>We begin by presenting the improvements achieved for each agent in the bar plots below:</p><ul><li><a href=https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning>DQN</a>, using mean squared error loss to minimize $\mathcal{L}_{\text{TD}}$ for DQN (as suggested in <a href=/posts/research/rl/revisiting_rainbow/>our recent Revisiting Rainbow paper</a>)</li></ul><img src=/posts/research/rl/mico/dqnBars.png alt="DQN bar plots" width=80% class=center><ul><li><a href=https://arxiv.org/abs/1710.02298>Rainbow</a></li></ul><img src=/posts/research/rl/mico/rainbowBars.png alt="Rainbow bar plots" width=80% class=center><ul><li><a href=https://arxiv.org/abs/1710.10044>QR-DQN</a></li></ul><img src=/posts/research/rl/mico/quantileBars.png alt="QR-DQN bar plots" width=80% class=center><ul><li><a href=https://arxiv.org/abs/1806.06923>IQN</a></li></ul><img src=/posts/research/rl/mico/iqnBars.png alt="IQN bar plots" width=80% class=center><ul><li><a href=https://papers.nips.cc/paper/2020/hash/2c6a0bae0f071cbbf0bb3d5b11d90a82-Abstract.html>M-IQN</a>; given that the authors had implemented their agent in TensorFlow (whereas our agents are in JAX), we have reimplemented M-IQN in JAX and run 5 independent runs (in contrast to the 3 run by the authors).</li></ul><img src=/posts/research/rl/mico/miqnBars.png alt="M-IQN bar plots" width=80% class=center><p>The figure below presents the aggregate normalized performance across all games; as can be seen, our loss is able to provide good improvements over the agents they are based on, suggesting that the MICo loss can help learn better representations for control.</p><img src=/posts/research/rl/mico/joinedNormalized.png alt="Human normalized scores" width=80% class=center><h4 id=dm-control-experiments>DM-Control experiments</h4><p>Additionally, we evaluated the MICo loss on twelve of the <a href=https://github.com/deepmind/dm_control>DM-Control suite from pixels environments</a>. As a base agent we used Soft Actor-Critic (SAC) from <a href=https://arxiv.org/abs/1812.05905>Haarnoja et al.</a> with the convolutional auto-encoder described by <a href=https://arxiv.org/abs/1910.01741>Yarats et al.</a>. We applied the MICo loss on the output of the auto-encoder (with $\alpha=1e-5$) and maintained all other parameters untouched. Recently, <a href=https://arxiv.org/abs/2006.10742>Zhang et al.</a> introduced DBC, which learns a dynamics and reward model on the output of the auto-encoder; their bisimulation loss uses the learned dynamics model in the computation of the Kantorovich distance between the next state transitions. We consider two variants of their algorithm: one which learns a stochastic dynamics model (DBC), and one which learns a deterministic dynamics model (DBC-Det). We replaced their bisimulation loss with the MICo loss (which, importantly, does not require a dynamics model) and kept all other parameters untouched. As the figure at the top of post illustrates, the best performance is achieved with SAC augmented with the MICo loss; additionally, replacing the bisimulation loss of DBC with the MICo loss is able to recover the performance of DBC to match that of SAC.</p><p>Here we present the learning curves for all the DM-Control environments run.</p><img src=/posts/research/rl/mico/allDMControl.png alt="All DM-Control environments" width=80% class=center><h2 id=conclusion>Conclusion</h2><p>In this paper, we have introduced the MICo distance, a notion of state similarity that can be learnt at scale and from samples. We have studied the theoretical properties of MICo, and proposed a new loss to make the non-zero self-distances of this diffuse metric compatible with function approximation, combining it with a variety of deep RL agents to obtain strong performance on the Arcade Learning Environment. In contrast to auxiliary losses that <em>implicitly</em> shape an agent&rsquo;s representation, MICo directly modifies the features learnt by a deep RL agent; our results indicate that this helps improve performance. To the best of our knowledge, this is the first time <em>directly</em> shaping the representation of RL agents has been successfully applied at scale. We believe this represents an interesting new approach to representation learning in RL; continuing to develop theory, algorithms and implementations for direct representation shaping in deep RL is an important and promising direction for future work.</p><h2 id=acknowledgements>Acknowledgements</h2><p>The authors would like to thank Gheorghe Comanici, Rishabh Agarwal, Nino
Vieillard, and Matthieu Geist for their valuable feedback on the paper and
experiments. Pablo Samuel Castro would like to thank Roman Novak and Jascha
Sohl-Dickstein for their help in getting angular distances to work stably!
Finally, the authors would like to thank the reviewers (both ICML'21 and
NeurIPS'21) for helping make this paper better.</p></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/mico.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/research/rl/tandem/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>The Difficulty of Passive Learning in Deep Reinforcement Learning</span></a></div><div class="col-md-6 next-article"><a href=/posts/musicode/ldd/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Losses, Dissonances, and Distortions</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var dsq=document.createElement("script");dsq.type="text/javascript";dsq.async=true;var disqus_shortname="does-not-exist";dsq.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a><ul><li><a href=#reinforcement-learning>Reinforcement learning</a></li><li><a href=#bisimulation-metrics>Bisimulation metrics</a></li><li><a href=#representation-learning-in-rl>Representation learning in RL</a></li></ul></li><li><a href=#limitations-of-bisimulation-metrics>Limitations of bisimulation metrics</a><ul><li><a href=#computational-complexity>Computational complexity</a></li><li><a href=#bias-under-sampled-transitions>Bias under sampled transitions.</a></li><li><a href=#lack-of-connection-to-non-optimal-policies>Lack of connection to non-optimal policies</a></li></ul></li><li><a href=#the-mico-distance>The MICo distance</a><ul><li><a href=#addressing-the-drawbacks-of-the-bisimulation-metric>Addressing the drawbacks of the bisimulation metric</a><ul><li><a href=#computational-complexity-1>Computational complexity</a></li><li><a href=#online-approximation>Online approximation</a></li><li><a href=#relationship-to-underlying-policy>Relationship to underlying policy</a></li></ul></li><li><a href=#diffuse-metrics>Diffuse metrics</a></li></ul></li><li><a href=#the-mico-loss>The MICo loss</a><ul><li><a href=#value-bound-gaps>Value bound gaps</a></li><li><a href=#state-features>State features</a></li></ul></li><li><a href=#empirical-evaluation>Empirical evaluation</a><ul><li><a href=#mico-loss-description>MICo loss description</a></li><li><a href=#results>Results</a><ul><li><a href=#ale-experiments>ALE experiments</a></li><li><a href=#dm-control-experiments>DM-Control experiments</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#acknowledgements>Acknowledgements</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul><a rel=me href=https://mathstodon.xyz/@psc>Mastodon</a></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>