<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on psc's website</title><link>https://psc-g.github.io/posts/research/rl/</link><description>Recent content in Reinforcement Learning on psc's website</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 19 Jun 2023 08:06:25 +0600</lastBuildDate><atom:link href="https://psc-g.github.io/posts/research/rl/index.xml" rel="self" type="application/rss+xml"/><item><title>The Dormant Neuron Phenomenon in Deep Reinforcement Learning</title><link>https://psc-g.github.io/posts/research/rl/redo/</link><pubDate>Mon, 19 Jun 2023 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/redo/</guid><description>We identify the dormant neuron phenomenon in deep reinforcement learning, where an agent&amp;rsquo;s network suffers from an increasing number of inactive neurons, thereby affecting network expressivity.
Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro*, Utku Evci*
This blogpost is a summary of our ICML 2023 paper. The code is available here. Many more results and analyses are available in the paper, so I encouraged you to check it out if interested!</description></item><item><title>Th State of Spars Train ng in D ep Re nforc m nt Le rn ng</title><link>https://psc-g.github.io/posts/research/rl/sparse_rl/</link><pubDate>Wed, 22 Jun 2022 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/sparse_rl/</guid><description>We perform a systematic investigation into applying a number of existing sparse training techniques on a variety of deep RL agents and environments, and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL.
Laura Graesser*, Utku Evci*, Erich Elsen, Pablo Samuel Castro
This blogpost is a summary of our ICML 2022 paper. The code is available here. Many more results and analyses are available in the paper, so I encouraged you to check it out if interested!</description></item><item><title>Deep Reinforcement Learning at the Edge of the Statistical Precipice</title><link>https://psc-g.github.io/posts/research/rl/precipice/</link><pubDate>Mon, 06 Dec 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/precipice/</guid><description>We argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. We advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results.</description></item><item><title>The Difficulty of Passive Learning in Deep Reinforcement Learning</title><link>https://psc-g.github.io/posts/research/rl/tandem/</link><pubDate>Tue, 26 Oct 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/tandem/</guid><description>We propose the &amp;ldquo;tandem learning&amp;rdquo; experimental design, where two RL agents are learning from identical data streams, but only one interacts with the environment to collect the data. We use this experiment design to study the empirical challenges of offline reinforcement learning.
Georg Ostrovski, Pablo Samuel Castro, Will Dabney
This blogpost is a summary of our NeurIPS 2021 paper. We provide two Tandem RL implementations: this one based on the DQN Zoo, and this one based on the Dopamine library.</description></item><item><title>MICo: Learning improved representations via sampling-based state similarity for Markov decision processes</title><link>https://psc-g.github.io/posts/research/rl/mico/</link><pubDate>Thu, 21 Oct 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/mico/</guid><description>We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents.
Pablo Samuel Castro*, Tyler Kastner*, Prakash Panangaden, and Mark Rowland
This blogpost is a summary of our NeurIPS 2021 paper. The code is available here.
The following figure gives a nice summary of the empirical gains our new loss provides, yielding an improvement on all of the Dopamine agents (left), as well as over Soft Actor-Critic and the DBC algorithm of Zhang et al.</description></item><item><title>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</title><link>https://psc-g.github.io/posts/research/rl/revisiting_rainbow/</link><pubDate>Mon, 24 May 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/revisiting_rainbow/</guid><description>We argue for the value of small- to mid-scale environments in deep RL for increasing scientific insight and help make our community more inclusive.
Johan S. Obando-Ceron and Pablo Samuel Castro
This is a summary of our paper which was accepted at the Thirty-eighth International Conference on Machine Learning (ICML'21). (An initial version was presented at the deep reinforcement learning workshop at NeurIPS 2020).
The code is available here.
You can see the Deep RL talk here.</description></item><item><title>Metrics and continuity in reinforcement learning</title><link>https://psc-g.github.io/posts/research/rl/metrics_continuity/</link><pubDate>Wed, 03 Feb 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/metrics_continuity/</guid><description>In this work we investigate the notion of &amp;ldquo;state similarity&amp;rdquo; in Markov decision processes. This concept is central to generalization in RL with function approximation.
Our paper was published at AAAI'21.
Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro
The text below was adapted from Charline&amp;rsquo;s twitter thread
In RL, we often deal with systems with large state spaces. We canâ€™t exactly represent the value of each of these states and need some type of generalization.</description></item><item><title>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning</title><link>https://psc-g.github.io/posts/research/rl/pse/</link><pubDate>Thu, 14 Jan 2021 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/pse/</guid><description>This paper was accepted as a spotlight at ICLR'21.
We propose a new metric and contrastive loss that comes equipped with theoretical and empirical results.
Policy Similarity Metric We introduce the policy similarity metric (PSM) which is based on bisimulation metrics. In contrast to bisimulation metrics (which is built on reward differences), PSMs are built on differences in optimal policies.
If we were to use this metric for policy transfer (as Doina Precup &amp;amp; I explored previously), we can upper-bound the difference between the optimal and the transferred policy:</description></item><item><title>2020 RL highlights</title><link>https://psc-g.github.io/posts/research/rl/2020highlights/</link><pubDate>Wed, 16 Dec 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/2020highlights/</guid><description>As part of TWiML &amp;rsquo;s AI Rewind series, I was asked to provide a list of reinforcement learning papers that were highlights for me in 2020. It&amp;rsquo;s been a difficult year for pretty much everyone, but it&amp;rsquo;s heartening to see that despite all the difficulties, interesting research still came out.
Given the size and breadth of the reinforcement learning research, as well as the fact that I was asked to do this at the end of NeurIPS and right before my vacation, I decided to apply the following rules in the selection:</description></item><item><title>Autonomous navigation of stratospheric balloons using reinforcement learning</title><link>https://psc-g.github.io/posts/research/rl/loon/</link><pubDate>Wed, 02 Dec 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/loon/</guid><description>In this work we, quite literally, take reinforcement learning to new heights! Specifically, we use deep reinforcement learning to help control the navigation of stratospheric balloons, whose purpose is to deliver internet to areas with low connectivity. This project is an ongoing collaboration with Loon.
It&amp;rsquo;s been incredibly rewarding to see reinforcement learning deployed successfully in a real setting. It&amp;rsquo;s also been terrific to work alongside such fantastic co-authors:
Marc G.</description></item><item><title>Scalable methods for computing state similarity in deterministic MDPs</title><link>https://psc-g.github.io/posts/research/rl/scalable/</link><pubDate>Fri, 22 Nov 2019 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/scalable/</guid><description>This post describes my paper Scalable methods for computing state similarity in deterministic MDPs, published at AAAI 2020. The code is available here.
Motivation We consider distance metrics between states in an MDP. Take the following MDP, where the goal is to reach the green cells:
Physical distance betweent states? Physical distance often fails to capture the similarity properties we&amp;rsquo;d like:
State abstractions Now imagine we add an exact copy of these states to the MDP (think of it as an additional &amp;ldquo;floor&amp;rdquo;):</description></item><item><title>Dopamine: A framework for flexible value-based reinforcement learning research</title><link>https://psc-g.github.io/posts/research/rl/dopamine/</link><pubDate>Mon, 27 Aug 2018 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/dopamine/</guid><description>Dopamine is a framework for flexible, value-based, reinforcement learning research. It was originally written in TensorFlow, but now all agents have been implemented in JAX.
You can read more about it in our github page and in our white paper.
Original Google AI blogpost.
We have a website where you can easily compare the performance of all the Dopamine agents, which I find really useful:
.
We also provide a set of Colaboratory notebooks that really help understand the framework:</description></item></channel></rss>