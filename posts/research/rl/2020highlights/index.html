<!doctype html><html><head><title>2020 RL highlights</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="2020 RL highlights"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-XXXXXXXXX-X','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a class=active href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/2020highlights/banner.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>December 16, 2020</p></div><div class=title><h1>2020 RL highlights</h1></div><div class=post-content id=post-content><p>As part of <a href=https://twimlai.com/>TWiML</a> &rsquo;s AI Rewind series, I was asked to provide a list of reinforcement learning papers that were highlights for me in 2020. It&rsquo;s been a difficult year for pretty much everyone, but it&rsquo;s heartening to see that despite all the difficulties, interesting research still came out.</p><p>Given the size and breadth of the reinforcement learning research, as well as the fact that I was asked to do this at the end of NeurIPS and right before my vacation, I decided to apply the following rules in the selection:</p><ul><li>Select only papers published in <a href=https://aaai.org/Conferences/AAAI-20/>AAAI</a>, <a href=https://iclr.cc/Conferences/2020>ICLR</a>, <a href=https://icml.cc/Conferences/2020>ICML</a>, or <a href=https://neurips.cc/>NeurIPS</a>. Like any good rule, there are a few exceptions :).</li><li>Select papers only from areas where I&rsquo;m most actively doing research. The <a href=#other>last section</a> is an exception to this rule.</li></ul><p>Due to time constraints, my process of selection was most likely not the best; if you feel there are papers I&rsquo;m omitting here, send them my way and I may add them. They are also presented here in no particular order, and for most I provide only a brief synposis taken from the papers themselves.
<font color=auburn>Unless written in auburn colour,</font> all texts below are taken from the source papers.</p><p>After having laid out all those disclaimers, hope this list proves useful!</p><p>You can see the interview with TWiML below:</p><h2 id=metrics--representations>Metrics / Representations</h2><p>One of my most active areas of research is investigating how to build good <em>representations</em> for learning. While there is no clear, and globally accepted, definition of what it means to have a good representation, what I take it to mean is:</p><ul><li>Has lower dimensionality than the original state space</li><li>Can be learned concurrently while policies are being improved upon</li><li>Can <em>generalize</em> well to unseen states</li><li>Has well-developed theoretical properties</li></ul><p>I first begin with some theoretical papers related to representation learning, and then I continue with metrics. I am of the belief that state (or state-action) metrics can help us with the last point and, if done carefully, can yield the other points as well. I begin with some papers that deal with state similarity measures, and transition into more &ldquo;traditional&rdquo; representation learning papers, which mostly make use of contrastive losses.</p><h2>Representations for Stable Off-Policy Reinforcement Learning</h2><p><em>Dibya Ghosh, Marc G. Bellemare</em></p><p><a href=http://proceedings.mlr.press/v119/ghosh20b.html>Paper</a></p><p>We formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice.</p><img src=/posts/research/rl/2020highlights/stable1.png width=50% class=center><p>Although positive-definite representations admit amenable optimization properties, such as invariance to reparametrization and monotonic convergence, they can only express value functions that satisfy a growth condition. Under on-policy sampling this growth condition is nonrestrictive, but as the policy deviates from the data distribution, the expressiveness of positive-definite representations reduces greatly.</p><h2>Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</h2><p><em>Simon S. Du, Sham M. Kakade, Ruosong Wang, Lin F. Yang</em></p><p><a href="https://openreview.net/forum?id=r1genAVKPB">Paper</a></p><p>Modern deep learning methods provide effective means to learn good
representations. However, is a good representation itself sufficient for sample
efficient reinforcement learning? Our main results provide sharp thresholds for
reinforcement learning methods, showing that there are hard limitations on what
constitutes good function approximation (in terms of the dimensionality of the
representation), where we focus on natural representational conditions relevant
to value-based, model-based, and policy-based learning. These lower bounds
highlight that having a good (value-based, model-based, or policy-based)
representation in and of itself is insufficient for efficient reinforcement
learning, unless the quality of this approximation passes certain hard
thresholds. Furthermore, our lower bounds also imply exponential separations on
the sample complexity between 1) value-based learning with perfect
representation and value-based learning with a good-but-not-perfect
representation, 2) value-based learning and policy-based learning, 3)
policy-based learning and supervised learning and 4) reinforcement learning and
imitation learning.</p><img src=/posts/research/rl/2020highlights/goodRepresentation.png width=50% class=center><h2>Scalable methods for computing state similarity in deterministic MDPs</h2><p><em>Pablo Samuel Castro</em></p><p><a href=https://arxiv.org/abs/1911.09291>Paper</a></p><p><img src=/posts/research/rl/scalable/banner.gif alt="Space Invaders and Bisimulationo" class=center>
<font color=auburn>Bisimulation metrics have some very pleasing theoretical properties, but are very expensive to compute and, until this year, were considered impractical for systems with very large (or continuous) state spaces. In this paper (published at AAAI) I introduce a mechanism for approximating these metrics using deep networks, even when we have large environments like Atar 2600 games, as long as transitions are deterministic.</p><p>You can read all about it in my <a href=/posts/research/rl/scalable/>post</a>.</font></p><h2>Learning Invariant Representations for Reinforcement Learning without Reconstruction</h2><p><em>Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, Sergey Levine</em></p><p><a href=https://arxiv.org/abs/2006.10742>Paper</a></p><font color=auburn>This paper has not been published in a peer-reviewed conference ([yet!](https://openreview.net/forum?id=-2FCwDKRREu)), but I include it here as it is a really nice followup to my AAAI paper just mentioned. They introduce Deep Bisimulation for Control (DBC), which aims to learn latent encodings whose $\ell_1$ distance respects the bisimulation metric.</font>
<img src=/posts/research/rl/2020highlights/invariantArchitecture.png width=50% class=center>
<font color=auburn>They overcome the deterministic assumption I had to make by noticing that if transitions are Gaussian, the 2-Wasserstein metric can be expressed in closed form. One of the things I really enjoyed about this paper is seeing my loss being used successfully for control (although their DBC proposal achieves better performance).</font>
<img src=/posts/research/rl/2020highlights/invariantComparison.png width=30% class=center><h2>Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery</h2><p><em>Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine</em></p><p><a href="https://openreview.net/forum?id=H1lmhaVtvr">Paper</a></p><p>We study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any
other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks
efficiently. We show that our method can learn to turn a valve with a real-world 9-DoF hand,
using raw image observations and just ten preference labels, without any other
supervision.</p><img src=/posts/research/rl/2020highlights/dynamicalEquation.png width=50% class=center>
<img src=/posts/research/rl/2020highlights/dynamicalAlgorithm.png width=50% class=center><h2>State Alignment-based Imitation Learning</h2><p><em>Fangchen Liu, Zhan Ling, Tongzhou Mu, Hao Su</em></p><p><a href="https://openreview.net/forum?id=rylrdxHFDr">Paper</a></p><p>We propose a novel state alignment based imitation learning method to train the imitator to follow the state sequences in expert
demonstrations as much as possible. The state alignment comes from both local and global perspectives and we combine them into a reinforcement learning
framework by a regularized policy update objective.</p><img src=/posts/research/rl/2020highlights/stateAlignment.png width=50% class=center><h2>Fast Task Inference with Variational Intrinsic Successor Features</h2><p><em>Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, Volodymyr Mnih</em></p><p><a href="https://openreview.net/forum?id=BJeAHkrYDS">Paper</a></p><p>It has been established that diverse behaviors spanning the controllable
subspace of a Markov decision process can be trained by rewarding a policy
for being distinguishable from other policies (Gregor et al., 2016; Eysenbach
et al., 2018; Warde-Farley et al., 2018). However, one limitation of this
formulation is the difficulty to generalize beyond the finite set of behaviors
being explicitly learned, as may be needed in subsequent tasks. Successor
features (Dayan, 1993; Barreto et al., 2017) provide an appealing solution
to this generalization problem, but require defining the reward function as
linear in some grounded feature space. In this paper, we show that these
two techniques can be combined, and that each method solves the other’s
primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework.</p><img src=/posts/research/rl/2020highlights/fast.png width=50% class=center><h2>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning</h2><p><em>Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare</em></p><p><a href=https://agarwl.github.io/pse/pdfs/paper.pdf>Paper</a></p><p><font color=auburn>This is another exemption to the first rule mentioned, but it was accepted and presented at the latest NeurIPS Deep RL workshop (poster) and the Workshop on Biological and Artificial RL (oral). We introduce the Policy Similarity Metric (PSM), which is based on bisimulation metrics but which replaces the differences in reward between states with the difference in optimal policies. This new metric is the reward-agnostic and can yield better generalization. We use this metric to introduce a new contrastive loss for learning representations: Policy Similarity Embeddings (PSEs). We demonstrate the effectiveness of this method on a number of challenging tasks.</font></p><p><font color=auburn>You can read more details in <a href=https://agarwl.github.io/pse/>Rishabh&rsquo;s blog post</a>.</font></p><img src=/posts/research/rl/2020highlights/psm.png width=50% class=center><h2>Contrastive Learning of Structured World Models</h2><p><em>Thomas Kipf, Elise van der Pol, Max Welling</em></p><p><a href="https://openreview.net/forum?id=H1gax6VtDB">Paper</a></p><p>CSWMs utilize a contrastive approach for representation learning in environments
with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This
allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional
environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation.</p><p><em>Our formulation of C-SWMs does not take into account stochasticity in environment transitions or observations, and hence is limited to fully deterministic worlds.</em></p><img src=/posts/research/rl/2020highlights/cswm.png width=50% class=center><h2>CURL: Contrastive Unsupervised Representations for Reinforcement Learning</h2><p><em>Michael Laskin, Aravind Srinivas, Pieter Abbeel</em></p><p><a href=http://proceedings.mlr.press/v119/laskin20a.html>Paper</a></p><p>CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features.</p><img src=/posts/research/rl/2020highlights/curl.png width=50% class=center><h2>Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning</h2><p><em>Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altché, Remi Munos, Mohammad Gheshlaghi Azar</em></p><p><a href=http://proceedings.mlr.press/v119/guo20g.html>Paper</a></p><p>PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics. Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics. In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more.</p><img src=/posts/research/rl/2020highlights/bootstrapRecurrent.png width=50% class=center>
<img src=/posts/research/rl/2020highlights/bootstrapArchitecture.png width=50% class=center><h2>Planning to Explore via Self-Supervised World Models</h2><p><em>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak</em></p><p><a href=http://proceedings.mlr.press/v119/sekar20a.html>Paper</a></p><p>We present Plan2Explore, a self-supervised reinforcement learning agent that
tackles both these challenges through a new approach to self-supervised exploration and fast
adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner.</p><img src=/posts/research/rl/2020highlights/plan2explore.png width=50% class=center><h2>Dream to Control: Learning Behaviors by Latent Imagination</h2><p><em>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi</em></p><p><a href="https://openreview.net/forum?id=S1lOTC4tDS">Paper</a></p><p>We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model.</p><img src=/posts/research/rl/2020highlights/dreamer.png width=50% class=center><h2>Model Based Reinforcement Learning for Atari</h2><p><em>Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski</em></p><p><a href="https://openreview.net/forum?id=S1xCPJHtDB">Paper</a></p><p>We explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning
(SimPLe), a complete model-based deep RL algorithm based on video prediction
models and present a comparison of several model architectures, including a novel
architecture that yields the best results in our setting.</p><img src=/posts/research/rl/2020highlights/simple.png width=50% class=center><h2>Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning</h2><p><em>Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee</em></p><p><a href="https://openreview.net/forum?id=HJgcvJBFvB">Paper</a></p><p>We propose a simple technique to improve a generalization ability of deep RL agents by
introducing a randomized (convolutional) neural network that randomly perturbs
input observations. It enables trained agents to adapt to new domains by learning
robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization.</p><img src=/posts/research/rl/2020highlights/netrand.png width=50% class=center><h2>Latent World Models For Intrinsically Motivated Exploration</h2><p><em>Aleksandr Ermolov, Nicu Sebe</em></p><p><a href=https://papers.nips.cc/paper/2020/hash/3c09bb10e2189124fdd8f467cc8b55a7-Abstract.html>Paper</a></p><p>We present a self-supervised representation learning method for image-based observations, which arranges embeddings respecting temporal distance of observations.
This representation is empirically robust to stochasticity and suitable for novelty
detection from the error of a predictive forward model. We consider episodic and
life-long uncertainties to guide the exploration. We propose to estimate the missing
information about the environment with the world model, which operates in the
learned latent space.</p><img src=/posts/research/rl/2020highlights/intrinsicExploration.png width=50% class=center><h2 id=understanding--evaluating-deep-rl>Understanding / evaluating deep RL</h2><p>I have become very interested in getting a better understanding of how deep networks interact with reinforcement learning. Most of the theory we have is limited to linear function approximators, but it is become increasingly evident that the, often overlooked, design decisions taken when setting up experiments can have a dramatic effect on performance.</p><h2>Revisiting Rainbow</h2><p><em>Johan S. Obando-Ceron and Pablo Samuel Castro</em></p><p><a href=https://arxiv.org/abs/2011.14826>Paper</a></p><p>I begin with another exception to the first rule. This is a paper we presented at the latest NeurIPS deep RL workshop. In it, we argue for the value of small- to mid-scale environments in deep RL for increasing scientific insight and help make our community more inclusive.</p><p>You can read all the details in <a href=/posts/research/rl/revisiting_rainbow/>my blog post</a>.</p><img src=/posts/research/rl/revisiting_rainbow/revisiting_rainbow.png width=100% class=center><h2>Measuring the Reliability of Reinforcement Learning Algorithms</h2><p><em>Stephanie C.Y. Chan, Samuel Fishman, Anoop Korattikara, John Canny, Sergio Guadarrama</em></p><p><a href="https://openreview.net/forum?id=SJlpYJBKvH">Paper</a></p><p>We propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary
statistical tests to enable rigorous comparisons on these metrics.</p><img src=/posts/research/rl/2020highlights/measuring1.png width=50% class=center>
<img src=/posts/research/rl/2020highlights/measuring2.png width=50% class=center><h2>Revisiting Fundamentals of Experience Replay</h2><p><em>William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney</em></p><p><a href=http://proceedings.mlr.press/v119/fedus20a.html>Paper</a></p><p>We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay — greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory.</p><img src=/posts/research/rl/2020highlights/revisitingReplay1.png width=50% class=center>
<img src=/posts/research/rl/2020highlights/revisitingReplay2.png width=50% class=center><h2>Behaviour Suite for Reinforcement Learning</h2><p><em>Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt</em></p><p><a href="https://openreview.net/forum?id=rygf-kSYwH">Paper</a></p><p>This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks.</p><img src=/posts/research/rl/2020highlights/bsuite.png width=50% class=center><h2>Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution</h2><p><em>Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, Sameer Singh</em></p><p><a href="https://openreview.net/forum?id=SJgzLkBKPB">Paper</a></p><p>Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps by balancing two aspects (specificity and relevance) that capture different desiderata of saliency. The first captures the impact of perturbation on the relative expected reward of the action to be explained. The second downweighs irrelevant features that alter the relative expected rewards of actions other than the action to be explained.</p><img src=/posts/research/rl/2020highlights/sarfa.png width=90% class=center><h2>Implementation Matters in Deep RL: A Case Study on PPO and TRPO</h2><p><em>Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry</em></p><p><a href="https://openreview.net/forum?id=r1etN1rtPB">Paper</a></p><p>We investigate the consequences of “code-level optimizations:” algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm.
Seemingly of secondary importance, such optimizations turn out to have a major
impact on agent behavior. Our results show that they (a) are responsible for most
of PPO’s gain in cumulative reward over TRPO, and (b) fundamentally change
how RL methods function.</p><img src=/posts/research/rl/2020highlights/impMatters.png width=90% class=center><h2 id=rl-in-the-real-world>RL in the real world</h2><p>We are all ultimately working towards having our methods become useful in the real world. In this section I present three papers I&rsquo;m on which do exactly this.</p><h2>Autonomous navigation of stratospheric balloons using reinforcement learning</h2><p><em>Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda and Ziyu Wang</em></p><p><a href=https://www.nature.com/articles/s41586-020-2939-8>Paper</a></p><p><font color=orange>In this work we, quite literally, take reinforcement learning to new heights! Specifically, we use deep reinforcement learning to help control the navigation of stratospheric balloons, whose purpose is to deliver internet to areas with low connectivity. This project is an ongoing collaboration with <a href=https://loon.com/>Loon</a>.</font></p><p><font color=orange>You can find links to more details in <a href=/posts/research/rl/loon/>my blog post</a>.</font></p><img src=/posts/research/rl/loon/loonAnimation.gif width=90% class=center><h2>Estimating Policy Functions in Payment Systems using Reinforcement Learning</h2><p><em>Pablo Samuel Castro, Ajit Desai, Han Du, Rodney Garratt, Francisco Rivadeneyra</em></p><p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3743017">Paper</a></p><p><font color=orange>This is another exception to the first rule. It was presented at the latest NeurIPS ML for Economic Policy workshop, and was awarded best empirical paper.</font></p><p><font color=orange>This paper is the result of an ongoing collaboration I have with the Bank of Canada. In this work we are demonstrating that RL can be a useful tool in simulating, evaluating, and ultimately understanding the complex dynamics of inter-bank high-value payment systems.</font></p><img src=/posts/research/rl/2020highlights/boc.jpeg width=90% class=center><h2>Agence: a dynamic film exploring multi-agent systems and human agency</h2>_Dante Camarena†, Pietro Gagliano, Alexander Bakogeorge, Nicholas Counter, Anuj Patel, Casey Blustein, Erin Ray, David Oppenheim, Laura Mingail, Kory W. Matthewson, Pablo Samuel Castro_<p><a href=http://agence.ai/s/AgenceNeurIPSCreativity2020.pdf>Paper</a></p><p><font color=orange>Yet another exception to my first rule, but I like this work because it is showing
how you can use RL (and ML in general) in a creative manner and deploy it to users around the world.
This was showcased at the Venice VR film festival this year, and was accepted as an oral at the NeurIPS
Machine Learning for Creativity & Design workshop. I also have a small <a href=/posts/research/creativity/agence/>blog post</a> about it.</font></p><p>Agence is a dynamic and interactive film authored by three parties: 1) the director, who
establishes the narrative structure and environment, 2) intelligent agents, using reinforcement learning
or scripted (hierarchical state machines) AI, and 3) the viewer, who can interact with the system to
affect the simulation. We trained RL agents in a multi-agent fashion to control some (or all, based on
user choice) of the agents in the film.</p><p><font color=orange>The following video does a great job at explaining more what it&rsquo;s about.</font></p><iframe src=https://player.vimeo.com/video/463920750 width=640 height=360 frameborder=0 allow="autoplay; fullscreen" allowfullscreen></iframe><p><a href=https://vimeo.com/463920750>The Story Behind Agence - A Dynamic Film</a> from <a href=https://vimeo.com/transformsai>Transitional Forms</a> on <a href=https://vimeo.com>Vimeo</a>.</p><h2 id=other>Other</h2><p>Finally, I&rsquo;m including a couple of papers that I thought were proposing something which, to me, seems quite innovative.</p><h2>Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions</h2><p><em>Michael Chang, Sid Kaushik, S. Matthew Weinberg, Tom Griffiths, Sergey Levine</em></p><p><a href=http://proceedings.mlr.press/v119/chang20b.html>Paper</a></p><p>We design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs.</p><img src=/posts/research/rl/2020highlights/vickrey.png width=50% class=center><h2>Munchausen Reinforcement Learning</h2><p><em>Nino Vieillard, Olivier Pietquin, Matthieu Geist</em></p><p><a href=https://proceedings.neurips.cc//paper_files/paper/2020/hash/2c6a0bae0f071cbbf0bb3d5b11d90a82-Abstract.html>Paper</a></p><p>Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with distributional methods on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very
little modifications to the original algorithm.</p><img src=/posts/research/rl/2020highlights/munchausen1.png width=90% class=center>
<img src=/posts/research/rl/2020highlights/munchausen2.png width=90% class=center>
<img src=/posts/research/rl/2020highlights/munchausen3.png width=90% class=center><h2>An operator view of policy gradient methods</h2><p><em>Dibya Ghosh, Marlos C. Machado, Nicolas Le Roux</em></p><p><a href=https://papers.nips.cc/paper/2020/hash/22eda830d1051274a2581d6466c06e6c-Abstract.html>Paper</a></p><p>We cast policy gradient methods as the repeated application of two operators: a
policy improvement operator $\mathcal{I}$, which maps any policy $\pi$ to a better one $\mathcal{I}\pi$, and
a projection operator $\mathcal{P}$, which finds the best approximation of $\mathcal{I}\pi$ in the set of
realizable policies. We use this framework to introduce operator-based versions of
well-known policy gradient methods such as REINFORCE and PPO, which leads to
a better understanding of their original counterparts. We also use the understanding
we develop of the role of $\mathcal{I}$ and $\mathcal{P}$ to propose a new global lower bound of the
expected return. This new perspective allows us to further bridge the gap between
policy-based and value-based methods, showing how REINFORCE and the Bellman
optimality operator, for example, can be seen as two sides of the same coin.</p><img src=/posts/research/rl/2020highlights/operator1.png width=90% class=center>
<img src=/posts/research/rl/2020highlights/operator2.png width=90% class=center><h2>What Can Learned Intrinsic Rewards Capture?</h2><p><em>Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver, Satinder Singh</em></p><p><a href=http://proceedings.mlr.press/v119/zheng20b.html>Paper</a></p><p>We propose a scalable meta-gradient framework for learning useful intrinsic reward functions across multiple lifetimes of experience. Through several proof-of-concept experiments, we show that it is feasible to learn and capture knowledge about long-term exploration and exploitation into a reward function.</p><img src=/posts/research/rl/2020highlights/intrinsic.png width=50% class=center></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/2020highlights.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/research/rl/loon/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Autonomous navigation of stratospheric balloons using reinforcement learning</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var dsq=document.createElement("script");dsq.type="text/javascript";dsq.async=true;var disqus_shortname="does-not-exist";dsq.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#metrics--representations>Metrics / Representations</a></li><li><a href=#understanding--evaluating-deep-rl>Understanding / evaluating deep RL</a></li><li><a href=#rl-in-the-real-world>RL in the real world</a></li><li><a href=#other>Other</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>