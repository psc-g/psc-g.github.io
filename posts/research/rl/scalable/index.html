<!doctype html><html><head><title>Scalable methods for computing state similarity in deterministic MDPs</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="Scalable methods for computing state similarity in deterministic MDPs"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/cme/>CME is A-OK</a></li><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/introduccion-a-transformers/>Intro a Transformers</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li><li><a href=/posts/mentoring/reviewing/>Tips for Reviewing Research Papers</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/phase1/>Phase 1</a><ul><li><a href=/posts/musicode/phase1/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/phase1/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/phase1/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/phase1/episode3/>3-Leitmotifs & Variables</a></li><li><a href=/posts/musicode/phase1/episode4/>4-Live Coding & Jazz</a></li><li><a href=/posts/musicode/phase1/episode5/>5-Repeats & Loops</a></li></ul></li><li><a href=/posts/musicode/introducing/>Introducing</a></li><li><a href=/posts/musicode/ldd/>Losses, Dissonances, and Distortions</a></li><li><a href=/posts/musicode/hallelagine/>Portrait of Hallelagine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/albums/>Albums</a></li><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/crosswords/>Crosswords</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li><li><a href=/posts/misc/pongday/>PongDay</a></li><li><a href=/posts/misc/yovoy/>yovoy</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/from_bbf_to_sss/>From BBF to SSS</a></li><li><a href=/posts/research/rl/atari_defense/>In defense of Atari</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/redo/>ReDo</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a class=active href=/posts/research/rl/scalable/>Scalable methods ...</a></li><li><a href=/posts/research/rl/sparse_rl/>SparseRL</a></li><li><a href=/posts/research/rl/precipice/>Statistical Precipice</a></li><li><a href=/posts/research/rl/tandem/>Tandem RL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/scalable/banner.gif)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>November 22, 2019</p></div><div class=title><h1>Scalable methods for computing state similarity in deterministic MDPs</h1></div><div class=post-content id=post-content><p>This post describes my paper <a href=https://arxiv.org/abs/1911.09291>Scalable methods for computing state similarity in deterministic MDPs</a>, published at <a href=https://aaai.org/Conferences/AAAI-20/>AAAI 2020</a>. The code is available <a href=https://github.com/google-research/google-research/tree/master/bisimulation_aaai2020>here</a>.</p><h2 id=motivation>Motivation</h2><p>We consider distance metrics between states in an MDP. Take the following MDP, where the goal is to reach the green cells:</p><img src=/posts/research/rl/scalable/sampleMDP.png alt="Sample MDP" width=50% class=center><h3 id=physical-distance-betweent-states>Physical distance betweent states?</h3><p>Physical distance often fails to capture the similarity properties we&rsquo;d like:</p><img src=/posts/research/rl/scalable/physicalDistance.png alt="Physical distance" width=50% class=center><h3 id=state-abstractions>State abstractions</h3><p>Now imagine we add an exact copy of these states to the MDP (think of it as an additional &ldquo;floor&rdquo;):</p><img src=/posts/research/rl/scalable/sampleMDPCopy.png alt="Sample MDP with copy" width=50% class=center><p>And imagine after each action the agent randomly transitions between these two floors:</p><img src=/posts/research/rl/scalable/sampleMDPCopy2.png alt="Sample MDP with copy and transitions" width=50% class=center><p>Notice that since the optimal policy is <em>identical</em> on both floors, we just doubled the state space for no reason! We should really be grouping together similar states.</p><h2 id=mdps>MDPs</h2><p>Consider the standard MDP defintion $\langle\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma\rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is the one-step reward function, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})$ is the next-state distribution function, and $\gamma\in [0, 1)$ is the discount factor.</p><h2 id=bisimulation>Bisimulation</h2><p>What we need is a notion of state distance that captures <em>behavioural indistinguishability</em>.</p><h3 id=equivalence-relations>Equivalence relations</h3><p>First, let me explain bisimulation relations, which was introduced by <a href=https://www.sciencedirect.com/science/article/pii/S0004370202003764>Givan, Dean, and Greig</a>.</p><p>Two states, $s$ and $t$, are considered <em>bisimlar</em> (denoted $s\sim t$) if, under all actions:</p><ol><li>They have equal immediate rewards:
$$ \forall a\in\mathcal{A}.\quad \mathcal{R}(s, a) = \mathcal{R}(t, a) $$</li><li>Transition with equal probability to bisimulation equivalence classes:
$$ \forall a\in\mathcal{A}, \forall c\in\mathcal{S}/_{\sim}.\quad\mathcal{P}(s, a)(c) := \sum_{s&rsquo;\in c}\mathcal{P}(s, a)(s&rsquo;) = \mathcal{P}(t, a)(c)$$</li></ol><p>This is often easiest to understand via an illustration. Consider the folloowing system:</p><img src=/posts/research/rl/scalable/bisimEquivExample.png alt="Example for bisimulation equivalence" width=50% class=center><p>A bisimulation equivalence relation would collapse the 8 states above into an equivalent 4-state MDP:</p><img src=/posts/research/rl/scalable/bisimEquivExampleCollapsed.png alt="Example for bisimulation equivalence, collapsed" width=50% class=center><p>But, unfortunately, bisimulation equivalence is brittle, as it&rsquo;s a 0/1 relationship!</p><h3 id=metrics>Metrics</h3><p>Bisimulation metrics generalize bisimulation relations, and quantify the <em>behavioural distance</em> between two states in an MDP. They are defined as any metric $d$ where $d(s, t) = 0 \iff s\sim t$. <a href=https://arxiv.org/abs/1207.4114>Ferns, Panangaden, and Precup</a> proved that the following operator admits a fixed point $d^{\sim}$, and this fixed point is a bisimulation metric:</p><p>$$ F(d)(s, t) = \max_{a\in\mathcal{A}}\left[ |R(s, a) - R(t, a)| + \gamma T_K(d)(P(s, a), P(t, a)) \right] $$</p><p>where $s,t\in\mathcal{S}$ are two states in the MDP, $\mathcal{A}$ is the action space, $R:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is the reward function, $P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})$ is the transition function, and $T_K(d)$ is the Kantorovich (also known as the Wasserstein, Earth Mover&rsquo;s, Sinkhorn, &mldr;) distance between two probability distributions under a state metric $d$.</p><p>These metrics have nice theoretical properties, such as: the bisimulation distance between two states is an upper-bound on their optimal value difference:</p><div>$$ | V^*(s) - V^*(t)| \leq d^{\sim}(s, t) $$</div><h2 id=shortcomings-and-solutions>Shortcomings and solutions</h2><p>Bisimulation metrics have three shortcomings I address in my paper.</p><h3 id=pessimism>Pessimism</h3><h4 id=problem>Problem</h4><p>They&rsquo;re inherently <em>pessimistic</em> (the max is considering the worst possible case!):
$$ F(d)(s, t) = {\color{red} \max_{a\in\mathcal{A}}}\left[ |R(s, a) - R(t, a)| + \gamma T_K(d)(P(s, a), P(t, a)) \right] $$
Take the following example MDP, where edge labels indicate the action (${a, b}$) and non-zero rewards ($[K]$).</p><img src=/posts/research/rl/scalable/counterexample.png alt=Counterexample width=25% class=center><p>When $\gamma = 0.9$, $V^∗(s) = V^∗(t) = 10K$, while $d^{\sim}(s, t) = 10K$, which shows that the bound mentioned in the last section can be made to be as loose as desired.</p><h4 id=solution>Solution</h4><p>I introduce <em>on-policy bisimulation metrics</em>, $d^{\pi}_{\sim}$. These are similar to regular bisimulation metrics, but are defined with respect to a policy $\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})$. In the paper I prove that
$$ |V^{\pi}(s) - V^{\pi}(t) | \leq d^{\pi}_{\sim}(s, t) $$</p><h3 id=computational-expense>Computational Expense</h3><h4 id=problem-1>Problem</h4><p>Bisimulation metrics have been traditionally solved via dynamic programming. But this can be very expensive, as it requires updating the metric estimate for all state-action pairs at every iteration! Note that this means computing the Kantorovich (which is an expensive linear program) $|\mathcal{S}|\times|\mathcal{A}|$ times at each iteration.</p><h4 id=solution-1>Solution</h4><p>Sampling! If we assume a deterministic MDP, I provide an update rule using sampled pairs of $\langle s, a, r, s&rsquo;\rangle$ transitions, and prove that this is guaranteed to converge to the true metric! See Theorem 4 in <a href=https://arxiv.org/pdf/1911.09291.pdf>the paper</a> for details.</p><h3 id=full-state-enumerability>Full state enumerability</h3><h4 id=problem-2>Problem</h4><p>Existing methods for computing/approximating bisimulation metrics require full state enumerability.</p><h4 id=solution-2>Solution</h4><p>Use neural networks! Consider a trained Rainbow agent, and take the penultimate layer as the representation ($\phi$):</p><img src=/posts/research/rl/scalable/trainedAgentRepr.png alt="Trained Rainbow agent representation" width=50% class=center><p>We can concatenate the representations of two states:</p><img src=/posts/research/rl/scalable/concatRepr.png alt="Concatenation of two representations" width=35% class=center><p>And feed them through a neural network, where we denote the output of the network as $\psi(s, t)$:</p><img src=/posts/research/rl/scalable/network.png alt="Concatenation of two representations, then fed through network" width=100% class=center><p>We define a target for regular bisimulation ($s&rsquo;$ and $t&rsquo;$ are the unique next states from $s$ and $t$, respectively):
$$ \mathbf{T}_{\theta}(s, t, a) = \max (\psi_{\theta}(s, t, a), |\mathcal{R}(s, a)-\mathcal{R}(t, a)| + \gamma\psi_{\theta}(s&rsquo;, t&rsquo;))$$</p><p>and for on-policy bisimulation:
$$ \mathbf{T}^{\pi}_{\theta}(s, t, a) = |\mathcal{R}^{\pi}(s)-\mathcal{R}^{\pi}(t)| + \gamma\psi^{\pi}_{\theta}(s&rsquo;, t&rsquo;))$$</p><p>Which is then incorporated into the loss (where $\mathcal{D}$ is the dataset, e.g. a replay buffer):
$$ \mathcal{L}^{(\pi)}_{s,t,a} = \mathbb{E}_{\mathcal{D}}\left(\mathbf{T}^{(\pi)}_{\theta}(s, t, a) - \psi^{(\pi)}_{\theta}(s, t, a)\right)^2 $$</p><h2 id=evaluate-on-atari-2600>Evaluate on Atari 2600</h2><p>Take a trained Rainbow agent and compute the distance from the first state:</p><img src=/posts/research/rl/scalable/firstState.png alt="First state in SpaceInvaders" width=25% class=center><p>to every other state:</p><img src=/posts/research/rl/scalable/si.gif alt="First state in SpaceInvaders to every other state" width=100% class=center><p>Let&rsquo;s see the results! Notice how the metric jumps when an alien ship is destroyed:</p><img src=/posts/research/rl/scalable/siDistances.gif alt="First state in SpaceInvaders to every other state, with distances" width=100% class=center><h2 id=citing>Citing</h2><p>The original tweet is <a href=https://twitter.com/pcastr/status/1197856799195619333>here</a>.</p><p>Please use the following BibTeX entry if you&rsquo;d like to cite this work:</p><pre tabindex=0><code>@inproceedings{castro20bisimulation,
  author    = {Pablo Samuel Castro},
  title     = {Scalable methods for computing state similarity in deterministic {M}arkov {D}ecision {P}rocesses},
  year      = {2020},
  booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
}
</code></pre></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/scalable.md><i class="fas fa-code-branch"></i>
Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/mentoring/resume/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i> Prev</span><br><span>Tips for preparing your resume</span></a></div><div class="col-md-6 next-article"><a href=/posts/art/cost-of-beauty/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>The Cost of Beauty</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="does-not-exist",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a><ul><li><a href=#physical-distance-betweent-states>Physical distance betweent states?</a></li><li><a href=#state-abstractions>State abstractions</a></li></ul></li><li><a href=#mdps>MDPs</a></li><li><a href=#bisimulation>Bisimulation</a><ul><li><a href=#equivalence-relations>Equivalence relations</a></li><li><a href=#metrics>Metrics</a></li></ul></li><li><a href=#shortcomings-and-solutions>Shortcomings and solutions</a><ul><li><a href=#pessimism>Pessimism</a><ul><li><a href=#problem>Problem</a></li><li><a href=#solution>Solution</a></li></ul></li><li><a href=#computational-expense>Computational Expense</a><ul><li><a href=#problem-1>Problem</a></li><li><a href=#solution-1>Solution</a></li></ul></li><li><a href=#full-state-enumerability>Full state enumerability</a><ul><li><a href=#problem-2>Problem</a></li><li><a href=#solution-2>Solution</a></li></ul></li></ul></li><li><a href=#evaluate-on-atari-2600>Evaluate on Atari 2600</a></li><li><a href=#citing>Citing</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Selected Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul><a rel=me href=https://sigmoid.social/@psc>Mastodon</a></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad()</script></body></html>