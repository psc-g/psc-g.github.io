<!doctype html><html><head><title>GANterpretations</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="GANterpretations"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-XXXXXXXXX-X','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><a href=/posts/musicode/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/episode3/>3-Leitmotifs & Variables</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/rl/>RL</a><ul><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/creativity/>Creativity</a><ul class=active><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a class=active href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/creativity/ganterpretations/gandy.gif)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>November 8, 2020</p></div><div class=title><h1>GANterpretations</h1></div><div class=post-content id=post-content><p>GANterpretations is an idea I published in <a href=https://arxiv.org/abs/2011.05158>this paper</a>, which was accepted to the <a href=https://neurips2020creativity.github.io/>4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020</a>. The code is available <a href=https://github.com/psc-g/ganterpretation>here</a>.</p><p>At a high level what it does is use the spectrogram of a piece of audio (from a video, for example) to &ldquo;draw&rdquo; a path in the latent space of a BigGAN.</p><p>The following video walks through the process:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/j2J8_Q9ZNa8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=gans>GANs</h2><p><a href=https://en.wikipedia.org/wiki/Generative_adversarial_network>GANs</a> are generative models trained to reproduce images from a given dataset. The way GANs work is they are trained to learn a <em>latent space</em> $ Z\in\mathbb{R}^d $, where each point $ z\in Z $ generates a unique image $ G(z) $, where $ G $ is the <em>generator</em> of the GAN. When trained properly, these latent spaces are learned in a structured manner, where nearby points generate similar images.</p><h3 id=biggan>BigGAN</h3><p>I use the <a href=https://arxiv.org/abs/1809.11096>BigGAN model</a>, which is a <em>class-conditional</em> GAN. In high-level terms, this means that the latent space is of the form $Z\times C$, where $Z\in\mathbb{R}^d$ and $C$ is a finite set of possible categories (such as <em>jellyfish</em>, <em>bike</em>, and <em>boa</em>). BigGAN has 1000 categories, and you can see generated samples from all of these <a href=https://psc-g.github.io/ganterpretation/all_samples.html>here</a>. For example, here are three samples from the <em>agama</em> category:</p><img src=/posts/research/creativity/ganterpretations/agama.png alt=Agama><h3 id=latent-space-interpolation>Latent space interpolation</h3><p>Properly trained GANs thus allow us to perform smooth interpolations between images in the same category $c\in C$ ($G((1 - \alpha) (z_1, c) + \alpha (z_2, c))$), and images from different categories $c_1,c_2\in C$ ($G((1 - \alpha) (z_1, c_1) + \alpha (z_2, c_2))$). Here we show a sample interpolation between three different types of generated dogs:</p><img src=/posts/research/creativity/ganterpretations/scooby_doo.gif alt="Dog interpolations"><p>We can use any signal in $\mathbb{R}^d$ for our $\alpha$ values, and in the next section I&rsquo;ll explain how I use the spectrogram from an audio file for this purpose.</p><h2 id=spectrograms>Spectrograms</h2><p>Given an audio file (could be the audio extracted from a video file), I use the <code>specgram</code> method from <code>matplotlib</code> to extract the spectrogram of an audio file. Most audio is in stereo, but I only use the left channel. The extracted spectrogram for the <a href=/posts/research/creativity/ganterpretations/#the-story-of-gandy>GANdy example</a> is as follows:</p><img src=/posts/research/creativity/ganterpretations/stft.png alt="STFT image"><p>This is an $ m \times n $ matrix, where $ m $ is the number of timesteps and $ n $ is the number of frequencies. Let $ F_t $ be the spectrogram at time $ t $; I compute the <a href=https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures>Total Variation distance</a> $ TV $ between $ F_{t-1} $ and $ F_t $ and we&rsquo;ll denote this value as $ TV(F_{t-1}, F_t) \in\mathbb{R} $.
The following image displays the TV distances for our running GANdy example.</p><img src=/posts/research/creativity/ganterpretations/tv.png alt="TV distances"><h3 id=inflection-points>Inflection points</h3><p>We &ldquo;slide&rdquo; through the TV distances across the timesteps to find <em>inflection points</em>. These are points along the timeline where there is a &ldquo;peak&rdquo; in the distances.</p><img src=/posts/research/creativity/ganterpretations/stft.gif alt="STFT video"><p>In words, what we are after are points that are <em>extrema</em> (i.e. the highest/lowest point within a certain window); note that we also add the first and last points in our timeline as inflection points. In our running GANdy example, we might get the following inflection points (this was generated with an inflection threshold of <code>1e-2</code>):</p><img src=/posts/research/creativity/ganterpretations/inflection_points.png alt="Inflection points"><p>In code, this is implemented as:</p><pre><code>def get_inflection_points(arr, threshold, absolute_threshold=8e-2,
                          type='both', rolling_length=200):
  inflection_points = [0]
  i = 0
  while i &lt; len(arr) - rolling_length - 1:
    prev_mean = np.mean(arr[i:i+rolling_length])
    curr_pos = i + rolling_length + 1
    next_mean = np.mean(arr[curr_pos+1:curr_pos+rolling_length+1])
    is_peak = (
            np.sign(arr[curr_pos] - prev_mean) ==
            np.sign(arr[curr_pos] - next_mean) and
            np.sign(arr[curr_pos] - arr[curr_pos-1]) ==
            np.sign(arr[curr_pos] - arr[curr_pos+1])) 
    if (is_peak and
        np.abs(arr[curr_pos] - prev_mean) &gt; threshold and
        np.abs(arr[curr_pos] - next_mean) &gt; threshold):
      if ((type == 'min' and (arr[curr_pos] &gt; arr[curr_pos-1] or
          arr[curr_pos] &gt; absolute_threshold)) or
          (type == 'max' and arr[curr_pos] &lt; arr[curr_pos-1])):
        i += rolling_length
        continue
      inflection_points.append(curr_pos)
      i += rolling_length
    else:
      i += 1
  inflection_points.append(len(arr) - 1)
  return np.array(inflection_points)
</code></pre><h3 id=alpha-values>Alpha values</h3><p>Now that we have inflection points the next step is to create $\alpha$ values between each pair of inflection points. To do this, we simply normalize the cumulative sum between inflection points, which will result in $\alpha$ values going from $0$ (at the first inflection point) to $1$ (at the second inflection point). Our running example would be:</p><img src=/posts/research/creativity/ganterpretations/alpha_values.png alt="Alpha values"><p>In code, this is simply:</p><pre><code>def get_alphas(arr):
  cumsum = np.cumsum(arr)
  total_sum = np.sum(arr)
  return cumsum / total_sum
</code></pre><h3 id=category-selection>Category selection</h3><p>Finally, we need to pick a category, from the 1000 possible <a href=https://psc-g.github.io/ganterpretation/all_samples.html>BigGAN categories</a>, for each of the inflection points. We can either specify these manually (which is especially useful when you&rsquo;re trying to have the images match the words in the audio), or have the system pick categories randomly.</p><h2 id=examples>Examples</h2><p>Here are some examples of some GANterpretations I&rsquo;ve generated. Click on the GIFs to open each video.</p><h3 id=bachbird>Bachbird</h3><p>I recorded myself playing Bachbird (my mashup of The Beatles' Blackbird and J.S. Bach&rsquo;s Prelude in C# Major), applied the GANterpretation process to it, and then combined the videos. The categories were not pre-selected (they were chosen randomly).</p><p><a href=https://youtu.be/oQI8zG0WNuI><img src=/posts/research/creativity/ganterpretations/bachbird.gif alt=BachBird></a></p><p><a href=https://twitter.com/pcastr/status/1181767820834721792>Original tweet</a></p><h3 id=the-story-of-gandy>The story of GANdy</h3><p>I selected the categories manually to match the narrative of this curious fellow.</p><p><a href=https://youtu.be/YelauzLHI6E><img src=/posts/research/creativity/ganterpretations/gandy.gif alt=GANdy></a></p><p><a href=https://twitter.com/pcastr/status/1213296573804941312>Original tweet</a></p><h3 id=latent-voyage>Latent Voyage</h3><p>I generated a melody using <a href=https://magenta.tensorflow.org/music-transformer>Music Transformer</a> and then made a GANterpretation video of the audio. The categories were manually selected around the theme of &ldquo;voyage&rdquo;.</p><p><a href=https://youtu.be/WH4b5-f6qoI><img src=/posts/research/creativity/ganterpretations/latent_voyage.gif alt="Latent Voyage"></a></p><p><a href=https://twitter.com/pcastr/status/1197373969474736129>Original tweet</a></p><h3 id=modern-primates>Modern Primates</h3><p>Another melody generated using <a href=https://magenta.tensorflow.org/music-transformer>Music Transformer</a> and a GANterpretation video of the audio. The categories were manually selected around the theme of &ldquo;primates&rdquo;.</p><p><a href=https://youtu.be/38Vi9XxKrrI><img src=/posts/research/creativity/ganterpretations/modern_primates.gif alt="Modern Primates"></a></p><p><a href=https://twitter.com/pcastr/status/1197517036211097601>Original tweet</a></p><h3 id=zappa>Zappa</h3><p>Frank Zappa talks about how much he likes music videos. The categories for the GANterpretation were chosen manually to match the words.</p><p><a href=https://youtu.be/D15kqdfA4no><img src=/posts/research/creativity/ganterpretations/zappa.gif alt=Zappa></a></p><p><a href=https://twitter.com/pcastr/status/1182227164843958272>Original tweet</a></p><h3 id=gan-leap>GAN leap</h3><p>One small step for man, one GAN leap for mankind. The categories were not pre-selected, they were chosen randomly (the rocket ship at the end was pure coincidence!).</p><p><a href=https://youtu.be/9iOR362occs><img src=/posts/research/creativity/ganterpretations/ganleap.gif alt=GANLeap></a></p><p><a href=https://twitter.com/pcastr/status/1217833237092950017>Original tweet</a></p></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/creativity/ganterpretations.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/research/rl/revisiting_rainbow/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</span></a></div><div class="col-md-6 next-article"><a href=/posts/mentoring/intro-to-rl/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Introduction to reinforcement learning</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var dsq=document.createElement("script");dsq.type="text/javascript";dsq.async=true;var disqus_shortname="does-not-exist";dsq.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#gans>GANs</a><ul><li><a href=#biggan>BigGAN</a></li><li><a href=#latent-space-interpolation>Latent space interpolation</a></li></ul></li><li><a href=#spectrograms>Spectrograms</a><ul><li><a href=#inflection-points>Inflection points</a></li><li><a href=#alpha-values>Alpha values</a></li><li><a href=#category-selection>Category selection</a></li></ul></li><li><a href=#examples>Examples</a><ul><li><a href=#bachbird>Bachbird</a></li><li><a href=#the-story-of-gandy>The story of GANdy</a></li><li><a href=#latent-voyage>Latent Voyage</a></li><li><a href=#modern-primates>Modern Primates</a></li><li><a href=#zappa>Zappa</a></li><li><a href=#gan-leap>GAN leap</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>