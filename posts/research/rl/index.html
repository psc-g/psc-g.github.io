<!doctype html><html><head><title>Reinforcement Learning</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=/assets/css/layouts/list.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/cme/>CME is A-OK</a></li><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/introduccion-a-transformers/>Intro a Transformers</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li><li><a href=/posts/mentoring/reviewing/>Tips for Reviewing Research Papers</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/phase1/>Phase 1</a><ul><li><a href=/posts/musicode/phase1/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/phase1/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/phase1/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/phase1/episode3/>3-Leitmotifs & Variables</a></li><li><a href=/posts/musicode/phase1/episode4/>4-Live Coding & Jazz</a></li><li><a href=/posts/musicode/phase1/episode5/>5-Repeats & Loops</a></li></ul></li><li><a href=/posts/musicode/introducing/>Introducing</a></li><li><a href=/posts/musicode/ldd/>Losses, Dissonances, and Distortions</a></li><li><a href=/posts/musicode/hallelagine/>Portrait of Hallelagine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/albums/>Albums</a></li><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/crosswords/>Crosswords</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li><li><a href=/posts/misc/pongday/>PongDay</a></li><li><a href=/posts/misc/yovoy/>yovoy</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a href=/posts/research/rl/from_bbf_to_sss/>From BBF to SSS</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/redo/>ReDo</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li><li><a href=/posts/research/rl/sparse_rl/>SparseRL</a></li><li><a href=/posts/research/rl/precipice/>Statistical Precipice</a></li><li><a href=/posts/research/rl/tandem/>Tandem RL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/research/rl/from_bbf_to_sss/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/from_bbf_to_sss/banner.png></div><div class=card-body><h5 class=card-title>From "Bigger, Better, Faster" to "Smaller, Sparser, Stranger"</h5><p class="card-text post-summary"><p>This is a post based on a talk I gave a few times in 2023. I had been meaning to put it in blog post form for over a year but kept putting it off&mldr; I guess better late than never. I think some of the ideas still hold, so hope some of you find it useful!</p><h2 id=bigger-better-faster>Bigger, better, faster</h2><p>In the <a href=https://www.nature.com/articles/nature14236>seminal DQN paper</a>, Mnih et al. demonstrated that reinforcement learning, when combined with neural networks as function approximators, could learn to play Atari 2600 games at superhuman levels. The DQN agent learned to do this over 200 million environment frames, which is roughly equivalent to 1000 hours of human gameplay&mldr;</p></p></div><div class=card-footer><span class=float-left>November 27, 2024</span>
<a href=/posts/research/rl/from_bbf_to_sss/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/redo/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/redo/overallIQM.png></div><div class=card-body><h5 class=card-title>The Dormant Neuron Phenomenon in Deep Reinforcement Learning</h5><p class="card-text post-summary"><p>We identify the dormant neuron phenomenon in deep reinforcement learning, where an agent&rsquo;s network suffers from an increasing number of inactive neurons, thereby affecting network expressivity.</p><p><em>Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro*, Utku Evci*</em></p><hr><p>This blogpost is a summary of our <a href=https://arxiv.org/abs/2302.12902>ICML 2023 paper</a>.
The code is available
<a href=https://github.com/google/dopamine/tree/master/dopamine/labs/redo>here</a>.
Many more results and analyses are available in the paper, so I encouraged you to
check it out if interested!</p><p>The following figure gives a nice summary of the overall findings of our work
(we are reporting the Interquantile Mean (IQM) as introduced in
<a href=https://arxiv.org/abs/2108.13264>our Statistical Precipice NeurIPS'21 paper</a>):</p></p></div><div class=card-footer><span class=float-left>June 19, 2023</span>
<a href=/posts/research/rl/redo/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/sparse_rl/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/sparse_rl/banner.gif></div><div class=card-body><h5 class=card-title>Th State of Spars Train ng in D ep Re nforc m nt Le rn ng</h5><p class="card-text post-summary"><p>We perform a systematic investigation into applying a number of existing sparse
training techniques on a variety of deep RL agents and environments, and
conclude by suggesting promising avenues for improving the effectiveness of
sparse training methods, as well as for advancing their use in DRL.</p><p><em>Laura Graesser*, Utku Evci*, Erich Elsen, Pablo Samuel Castro</em></p><hr><p>This blogpost is a summary of our <a href=https://arxiv.org/abs/2206.10369>ICML 2022 paper</a>.
The code is available
<a href=https://github.com/google-research/rigl/tree/master/rigl/rl>here</a>.
Many more results and analyses are available in the paper, so I encouraged you to
check it out if interested!</p></p></div><div class=card-footer><span class=float-left>June 22, 2022</span>
<a href=/posts/research/rl/sparse_rl/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/precipice/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/precipice/banner.png></div><div class=card-body><h5 class=card-title>Deep Reinforcement Learning at the Edge of the Statistical Precipice</h5><p class="card-text post-summary"><p>We argue that reliable evaluation in the few run deep RL regime cannot ignore
the uncertainty in results without running the risk of slowing down progress in
the field. We illustrate this point using a case study on the Atari 100k
benchmark, where we find substantial discrepancies between conclusions drawn
from point estimates alone versus a more thorough statistical analysis.
We advocate for reporting interval estimates of aggregate performance and
propose performance profiles to account for the variability in results, as well
as present more robust and efficient aggregate metrics, such as interquartile
mean scores, to achieve small uncertainty in results.</p></p></div><div class=card-footer><span class=float-left>December 6, 2021</span>
<a href=/posts/research/rl/precipice/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/tandem/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/tandem/heldHein.png></div><div class=card-body><h5 class=card-title>The Difficulty of Passive Learning in Deep Reinforcement Learning</h5><p class="card-text post-summary"><p>We propose the &ldquo;tandem learning&rdquo; experimental design, where two RL agents are
learning from identical data streams, but only one interacts with the
environment to collect the data. We use this experiment design to study the
empirical challenges of offline reinforcement learning.</p><p><em>Georg Ostrovski, Pablo Samuel Castro, Will Dabney</em></p><p>This blogpost is a summary of our <a href=https://arxiv.org/abs/2110.14020>NeurIPS 2021 paper</a>.
We provide two Tandem RL implementations:
<a href=https://github.com/deepmind/deepmind-research/tree/master/tandem_dqn>this one</a>
based on the <a href=https://github.com/deepmind/dqn_zoo>DQN Zoo</a>, and
<a href=https://github.com/google/dopamine/tree/master/dopamine/labs/tandem_dqn>this one</a>
based on the <a href=https://github.com/google/dopamine>Dopamine library</a>.</p></p></div><div class=card-footer><span class=float-left>October 26, 2021</span>
<a href=/posts/research/rl/tandem/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/mico/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/mico/banner.gif></div><div class=card-body><h5 class=card-title>MICo: Learning improved representations via sampling-based state similarity for Markov decision processes</h5><p class="card-text post-summary"><p>We present a new behavioural distance over the state space of a Markov
decision process, and demonstrate the use of this distance as an effective
means of shaping the learnt representations of deep reinforcement learning
agents.</p><p><em>Pablo Samuel Castro*, Tyler Kastner*, Prakash Panangaden, and Mark Rowland</em></p><center><iframe width=560 height=315 src=https://www.youtube.com/embed/CWKv2R30c9E title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center><hr><p>This blogpost is a summary of our <a href=https://arxiv.org/abs/2106.08229>NeurIPS 2021 paper</a>.
The code is available
<a href=https://github.com/google-research/google-research/tree/master/mico>here</a>.</p><p>The following figure gives a nice summary of the empirical gains our new loss
provides, yielding an improvement on all of the
<a href=https://github.com/google/dopamine>Dopamine</a> agents (left), as well as over Soft
Actor-Critic and the DBC algorithm of <a href=https://arxiv.org/abs/2006.10742>Zhang et al., ICLR 2021</a> (right).
In both cases we are reporting the Interquantile Mean as introduced in
<a href=https://arxiv.org/abs/2108.13264>our Statistical Precipice NeurIPS'21 paper</a>.</p></p></div><div class=card-footer><span class=float-left>October 21, 2021</span>
<a href=/posts/research/rl/mico/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/revisiting_rainbow/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/revisiting_rainbow/revisiting_rainbow.png></div><div class=card-body><h5 class=card-title>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</h5><p class="card-text post-summary"><p>We argue for the value of small- to mid-scale environments in deep RL for increasing scientific insight and help make our community more inclusive.</p><p><em>Johan S. Obando-Ceron and Pablo Samuel Castro</em></p><p>This is a summary of our <a href=https://arxiv.org/abs/2011.14826>paper</a> which was accepted at the
<a href=https://icml.cc/>Thirty-eighth International Conference on Machine Learning (ICML'21)</a>. (An initial version was presented at the <a href=https://sites.google.com/corp/view/deep-rl-workshop-neurips2020/home>deep reinforcement learning workshop at NeurIPS 2020</a>).</p><p>The code is available <a href=https://github.com/JohanSamir/revisiting_rainbow>here</a>.</p><p>You can see the Deep RL talk <a href=https://slideslive.com/38941329/revisiting-rainbow-promoting-more-insightful-and-inclusive-deep-reinforcement-learning-research>here</a>.</p></p></div><div class=card-footer><span class=float-left>May 24, 2021</span>
<a href=/posts/research/rl/revisiting_rainbow/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/metrics_continuity/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/metrics_continuity/banner.png></div><div class=card-body><h5 class=card-title>Metrics and continuity in reinforcement learning</h5><p class="card-text post-summary"><p>In this work we investigate the notion of &ldquo;state similarity&rdquo; in Markov decision processes. This concept is central to generalization in RL with function approximation.</p><p><a href=https://arxiv.org/abs/2102.01514>Our paper</a> was published at AAAI'21.</p><p><em>Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro</em></p><p>The text below was adapted from <a href=https://twitter.com/charlinelelan/status/1357006401952972808>Charline&rsquo;s twitter thread</a></p><p>In RL, we often deal with systems with large state spaces. We can’t exactly represent the value of each of these states and need some type of generalization. One way to do that is to look at structured representations in which similar states are assigned similar predictions.</p></p></div><div class=card-footer><span class=float-left>February 3, 2021</span>
<a href=/posts/research/rl/metrics_continuity/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/pse/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/pse/pse.png></div><div class=card-body><h5 class=card-title>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning</h5><p class="card-text post-summary"><p>This <a href=https://arxiv.org/abs/2101.05265>paper</a> was accepted as a spotlight at ICLR'21.</p><p>We propose a new metric and contrastive loss that comes equipped with theoretical and empirical results.</p><img src=/posts/research/rl/pse/jumpy.png alt="Jumping task" width=50% class=center><h2 id=policy-similarity-metric>Policy Similarity Metric</h2><p>We introduce the policy similarity metric (PSM) which is based on <a href=https://arxiv.org/abs/1207.4114>bisimulation metrics</a>.
In contrast to bisimulation metrics (which is built on reward differences), PSMs are built on differences in optimal policies.</p><img src=/posts/research/rl/pse/psm.png alt="Policy similarity metric" width=80% class=center><p>If we were to use this metric for policy transfer (as Doina Precup & I <a href=http://ojs.aaai.org/index.php/AAAI/article/view/7751>explored previously</a>), we can upper-bound the difference between the optimal and the transferred policy:</p></p></div><div class=card-footer><span class=float-left>January 14, 2021</span>
<a href=/posts/research/rl/pse/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/2020highlights/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/2020highlights/banner.png></div><div class=card-body><h5 class=card-title>2020 RL highlights</h5><p class="card-text post-summary"><p>As part of <a href=https://twimlai.com/>TWiML</a> &rsquo;s AI Rewind series, I was asked to provide a list of reinforcement learning papers that were highlights for me in 2020. It&rsquo;s been a difficult year for pretty much everyone, but it&rsquo;s heartening to see that despite all the difficulties, interesting research still came out.</p><p>Given the size and breadth of the reinforcement learning research, as well as the fact that I was asked to do this at the end of NeurIPS and right before my vacation, I decided to apply the following rules in the selection:</p></p></div><div class=card-footer><span class=float-left>December 16, 2020</span>
<a href=/posts/research/rl/2020highlights/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/loon/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/loon/loonAnimation.gif></div><div class=card-body><h5 class=card-title>Autonomous navigation of stratospheric balloons using reinforcement learning</h5><p class="card-text post-summary"><p>In this work we, quite literally, take reinforcement learning to new heights! Specifically, we use deep reinforcement learning to help control the navigation of stratospheric balloons, whose purpose is to deliver internet to areas with low connectivity. This project is an ongoing collaboration with <a href=https://loon.com/>Loon</a>.</p><p>It&rsquo;s been incredibly rewarding to see reinforcement learning deployed successfully in a real setting. It&rsquo;s also been terrific to work alongside such fantastic co-authors:<br><em>Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, Ziyu Wang</em></p></p></div><div class=card-footer><span class=float-left>December 2, 2020</span>
<a href=/posts/research/rl/loon/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/research/rl/scalable/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/posts/research/rl/scalable/banner.gif></div><div class=card-body><h5 class=card-title>Scalable methods for computing state similarity in deterministic MDPs</h5><p class="card-text post-summary"><p>This post describes my paper <a href=https://arxiv.org/abs/1911.09291>Scalable methods for computing state similarity in deterministic MDPs</a>, published at <a href=https://aaai.org/Conferences/AAAI-20/>AAAI 2020</a>. The code is available <a href=https://github.com/google-research/google-research/tree/master/bisimulation_aaai2020>here</a>.</p><h2 id=motivation>Motivation</h2><p>We consider distance metrics between states in an MDP. Take the following MDP, where the goal is to reach the green cells:</p><img src=/posts/research/rl/scalable/sampleMDP.png alt="Sample MDP" width=50% class=center><h3 id=physical-distance-betweent-states>Physical distance betweent states?</h3><p>Physical distance often fails to capture the similarity properties we&rsquo;d like:</p><img src=/posts/research/rl/scalable/physicalDistance.png alt="Physical distance" width=50% class=center><h3 id=state-abstractions>State abstractions</h3><p>Now imagine we add an exact copy of these states to the MDP (think of it as an additional &ldquo;floor&rdquo;):</p></p></div><div class=card-footer><span class=float-left>November 22, 2019</span>
<a href=/posts/research/rl/scalable/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator><ul class="pagination pagination-default"><li class="page-item disabled"><a aria-disabled=true aria-label=First class=page-link role=button tabindex=-1><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a aria-disabled=true aria-label=Previous class=page-link role=button tabindex=-1><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a aria-current=page aria-label="Page 1" class=page-link role=button>1</a></li><li class=page-item><a href=/posts/research/rl/page/2/ aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/posts/research/rl/page/2/ aria-label=Next class=page-link role=button><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/research/rl/page/2/ aria-label=Last class=page-link role=button><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Selected Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul><a rel=me href=https://sigmoid.social/@psc>Mastodon</a></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=/assets/js/list.js></script></body></html>