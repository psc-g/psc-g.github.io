<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on psc's website</title><link>https://psc-g.github.io/posts/research/rl/</link><description>Recent content in Reinforcement Learning on psc's website</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Dec 2020 08:06:25 +0600</lastBuildDate><atom:link href="https://psc-g.github.io/posts/research/rl/index.xml" rel="self" type="application/rss+xml"/><item><title>Autonomous navigation of stratospheric balloons using reinforcement learning</title><link>https://psc-g.github.io/posts/research/rl/loon/</link><pubDate>Wed, 02 Dec 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/loon/</guid><description>In this work we, quite literally, take reinforcement learning to new heights! Specifically, we use deep reinforcement learning to help control the navigation of stratospheric balloons, whose purpose is to deliver internet to areas with low connectivity. This project is an ongoing collaboration with Loon.
It&amp;rsquo;s been incredibly rewarding to see reinforcement learning deployed successfully in a real setting. It&amp;rsquo;s also been terrific to work alongside such fantastic co-authors: Marc G.</description></item><item><title>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</title><link>https://psc-g.github.io/posts/research/rl/revisiting_rainbow/</link><pubDate>Mon, 30 Nov 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/revisiting_rainbow/</guid><description>We argue for the value of small- to mid-scale environments in deep RL for increasing scientific insight and help make our community more inclusive.
Johan S. Obando-Ceron and Pablo Samuel Castro
This is a summary of our paper which will be presented in the deep reinforcement learning workshop at NeurIPS 2020.
The code is available here.
You can see the Deep RL talk here.
Introduction Since the introduction of DQN (Mnih et al.</description></item><item><title>Scalable methods for computing state similarity in deterministic MDPs</title><link>https://psc-g.github.io/posts/research/rl/scalable/</link><pubDate>Fri, 22 Nov 2019 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/scalable/</guid><description>This post describes my paper Scalable methods for computing state similarity in deterministic MDPs, published at AAAI 2020. The code is available here.
Motivation We consider distance metrics between states in an MDP. Take the following MDP, where the goal is to reach the green cells:
Physical distance betweent states? Physical distance often fails to capture the similarity properties we&amp;rsquo;d like:
State abstractions Now imagine we add an exact copy of these states to the MDP (think of it as an additional &amp;ldquo;floor&amp;rdquo;):</description></item><item><title>Dopamine: A framework for flexible value-based reinforcement learning research</title><link>https://psc-g.github.io/posts/research/rl/dopamine/</link><pubDate>Mon, 27 Aug 2018 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/rl/dopamine/</guid><description>Dopamine is a framework for flexible, value-based, reinforcement learning research. It was originally written in TensorFlow, but now all agents have been implemented in JAX.
You can read more about it in our github page and in our white paper.
Original Google AI blogpost.
We have a website where you can easily compare the performance of all the Dopamine agents, which I find really useful:
.
We also provide a set of Colaboratory notebooks that really help understand the framework:</description></item></channel></rss>