<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Other on psc's website</title><link>https://psc-g.github.io/posts/research/other/</link><description>Recent content in Other on psc's website</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 16 Sep 2020 08:06:25 +0600</lastBuildDate><atom:link href="https://psc-g.github.io/posts/research/other/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigging the Lottery: Making All Tickets Winners</title><link>https://psc-g.github.io/posts/research/other/rigl/rigl/</link><pubDate>Wed, 16 Sep 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/other/rigl/rigl/</guid><description>Rigging the Lottery: Making All Tickets Winners is a paper published at ICML 2020 with Utku Evci, Trevor Gale, Jacob Menick, and Erich Elsen, where we introduce an algorithm for training sparse neural networks that uses a fixed parameter count and computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods.
You can read more about it in the paper and in our blog post.</description></item></channel></rss>