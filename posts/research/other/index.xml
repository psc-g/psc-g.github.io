<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Other on psc's website</title><link>https://psc-g.github.io/posts/research/other/</link><description>Recent content in Other on psc's website</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 16 Sep 2020 08:06:25 +0600</lastBuildDate><atom:link href="https://psc-g.github.io/posts/research/other/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigging the Lottery: Making All Tickets Winners</title><link>https://psc-g.github.io/posts/research/other/rigl/rigl/</link><pubDate>Wed, 16 Sep 2020 08:06:25 +0600</pubDate><guid>https://psc-g.github.io/posts/research/other/rigl/rigl/</guid><description>&lt;p>&lt;a href="https://proceedings.icml.cc/static/paper_files/icml/2020/287-Paper.pdf">Rigging the Lottery: Making All Tickets Winners&lt;/a> is a paper published at &lt;a href="https://icml.cc/Conferences/2020">ICML 2020&lt;/a> with Utku Evci, Trevor Gale, Jacob Menick, and Erich Elsen, where we introduce an algorithm for training sparse neural networks that uses a fixed parameter count and computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods.&lt;/p>
&lt;p>You can read more about it in the paper and in our &lt;a href="https://ai.googleblog.com/2020/09/improving-sparse-training-with-rigl.html">blog post&lt;/a>.&lt;/p>
&lt;img src="https://psc-g.github.io/posts/research/other/rigl/rigl.gif"
 
 alt="RigL"
 
 
 width="25%"
 
 
 
 
 class="center"
 
></description></item></channel></rss>