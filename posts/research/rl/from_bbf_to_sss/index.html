<!doctype html><html><head><title>From "Bigger, Better, Faster" to "Smaller, Sparser, Stranger"</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content='From "Bigger, Better, Faster" to "Smaller, Sparser, Stranger"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/cme/>CME is A-OK</a></li><li><a href=/posts/mentoring/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/introduccion-a-transformers/>Intro a Transformers</a></li><li><a href=/posts/mentoring/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/>Tips for Interviewing at Google</a></li><li><a href=/posts/mentoring/reviewing/>Tips for Reviewing Research Papers</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/>MUSICODE</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/musicode/phase1/>Phase 1</a><ul><li><a href=/posts/musicode/phase1/introducing/>0-Introducing</a></li><li><a href=/posts/musicode/phase1/episode1/>1-Musical Note & Computation</a></li><li><a href=/posts/musicode/phase1/episode2/>2-Bits & Semitones</a></li><li><a href=/posts/musicode/phase1/episode3/>3-Leitmotifs & Variables</a></li><li><a href=/posts/musicode/phase1/episode4/>4-Live Coding & Jazz</a></li><li><a href=/posts/musicode/phase1/episode5/>5-Repeats & Loops</a></li></ul></li><li><a href=/posts/musicode/introducing/>Introducing</a></li><li><a href=/posts/musicode/ldd/>Losses, Dissonances, and Distortions</a></li><li><a href=/posts/musicode/hallelagine/>Portrait of Hallelagine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/albums/>Albums</a></li><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/covid-music/>Covid Music</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li><li><a href=/posts/misc/crosswords/>Crosswords</a></li><li><a href=/posts/misc/origins/>Origins of April Fool's Day</a></li><li><a href=/posts/misc/pongday/>PongDay</a></li><li><a href=/posts/misc/yovoy/>yovoy</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/2020highlights/>2020 RL Highlights</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings</a></li><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a href=/posts/research/rl/loon/>Flying balloons with RL</a></li><li><a class=active href=/posts/research/rl/from_bbf_to_sss/>From BBF to SSS</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics & continuity in RL</a></li><li><a href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/redo/>ReDo</a></li><li><a href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li><li><a href=/posts/research/rl/sparse_rl/>SparseRL</a></li><li><a href=/posts/research/rl/precipice/>Statistical Precipice</a></li><li><a href=/posts/research/rl/tandem/>Tandem RL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/agence/>Agence, a dynamic film</a></li><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/from_bbf_to_sss/banner.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>November 27, 2024</p></div><div class=title><h1>From "Bigger, Better, Faster" to "Smaller, Sparser, Stranger"</h1></div><div class=post-content id=post-content><p>This is a post based on a talk I gave a few times in 2023. I had been meaning to put it in blog post form for over a year but kept putting it off&mldr; I guess better late than never. I think some of the ideas still hold, so hope some of you find it useful!</p><h2 id=bigger-better-faster>Bigger, better, faster</h2><p>In the <a href=https://www.nature.com/articles/nature14236>seminal DQN paper</a>, Mnih et al. demonstrated that reinforcement learning, when combined with neural networks as function approximators, could learn to play Atari 2600 games at superhuman levels. The DQN agent learned to do this over 200 million environment frames, which is roughly equivalent to 1000 hours of human gameplay&mldr;</p><p><em>This is still quite a lot!</em> We can&rsquo;t always assume cheap simulators are available; sample efficiency is key!</p><p><a href=https://arxiv.org/abs/1903.00374>Kaiser et al.</a> introduced the Atari 100k benchmark which evaluates agents for 100k agent interactions (or 400k environment frames), equivalent to roughly 2 hours of human gameplay.</p><p>In <a href=https://arxiv.org/abs/2305.19452>Bigger, Better, Faster: Human-level Atari with human-level efficiency</a>, our ICML 2023 paper, we introduced BBF, a model-free agent which was able to achieve superhuman performance with only 100k agent interactions:</p><img src=/posts/research/rl/from_bbf_to_sss/bbf_fig1.png alt="BBF over time" width=50% class=center><p>To achieve this, we combined a number of recent techniques from the literature, including <a href="https://openreview.net/forum?id=uCQfPZwRaUu">SPR</a> and <a href="https://openreview.net/forum?id=OpC-9aBBVJe">network resets</a>. The following figure ablates each of the components, indicating which proved to be most important.</p><img src=/posts/research/rl/from_bbf_to_sss/bbf_ablations.png alt="BBF ablations" width=50% class=center><h2 id=yes-but-why>Yes, but why?</h2><p>The natural question is: <em>why</em> do we need all these components? We will explore some of the main components in separate sections below.</p><h3 id=self-supervised-learning>Self-supervised learning</h3><p><a href="https://openreview.net/forum?id=uCQfPZwRaUu">SPR</a> was the base agent for BBF, which relies on self-supervised learning (BYOL, specifically). What does self-supervised learning buy us?</p><blockquote><p>Good representations!</p></blockquote><p>This is a topic near and dear to me, as I&rsquo;ve done a lot of work on it, mostly based on <a href=https://arxiv.org/abs/1207.4114>bisimulation metrics</a>. To read more about my work in this space, see:</p><ul><li><a href=/posts/research/rl/scalable/>Scalable methods for computing state similarity in deterministic MDPs</a></li><li><a href=/posts/research/rl/mico/>MICo</a></li><li><a href=/posts/research/rl/pse/>Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning</a></li><li><a href="https://openreview.net/forum?id=nHfPXl1ly7">A Kernel Perspective on Behavioural Metrics for Markov Decision Processes</a></li><li><a href=/posts/research/rl/metrics_continuity/>Metrics and continuity in reinforcement learning</a></li></ul><h3 id=network-resets>Network resets</h3><p>Another critical component of BBF is the use of network resets, as used in <a href="https://openreview.net/forum?id=OpC-9aBBVJe">SR-SPR</a> (and first introduced in <a href=https://arxiv.org/abs/2205.07802>The Primacy Bias in Deep Reinforcement Learning</a>). Why do resets help so much?</p><blockquote><p>Mitigate overfitting and maintain plasticity!</p></blockquote><p>There is a growing body of work demonstrating that RL networks tend to overfit to their most recent data, and this seems to cause a loss in plasticity (which is essential for online RL). Some works exploring this idea are:</p><ul><li><a href=https://arxiv.org/abs/2006.02243>The Value-Improvement Path: Towards Better Representations for Reinforcement Learning</a></li><li><a href=https://arxiv.org/abs/2303.01486>Understanding plasticity in neural networks</a></li><li><a href=https://arxiv.org/abs/2302.12902>The Dormant Neuron Phenomenon in Deep Reinforcement Learning</a></li><li><a href=https://arxiv.org/abs/2406.02596>Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks</a></li><li><a href=https://arxiv.org/abs/2407.01800>Normalization and effective learning rates in reinforcement learning</a></li></ul><h3 id=larger-networks>Larger networks</h3><p>The use of larger networks was important to achieve BBF&rsquo;s ultimate performance; but scaling networks in RL is really hard. More generally, the takeaway is:</p><blockquote><p>Architectures matter!</p></blockquote><p>As evidence of this, consider the following plot, evaluating the impact of switching from the original DQN architecture to the <a href=https://arxiv.org/abs/1802.01561>IMPALA resnet</a> architecture, and nothing else!</p><img src=/posts/research/rl/from_bbf_to_sss/impala_rocks.png alt="Impala rocks!" width=50% class=center><p>There is a growing body of work exploring alternate architectures for deep RL agents. Here are a few recent ones that I have been involved in.</p><ul><li><a href=https://arxiv.org/abs/2206.10369>The State of Sparse Training in Deep Reinforcement Learning</a></li><li><a href=https://arxiv.org/abs/2402.12479>In value-based deep reinforcement learning, a pruned network is a good network</a></li><li><a href="https://openreview.net/forum?id=X9VMhfFxwn">Mixtures of Experts Unlock Parameter Scaling for Deep RL</a></li><li><a href=https://arxiv.org/abs/2410.01930>Don&rsquo;t flatten, tokenize! Unlocking the key to SoftMoE&rsquo;s efficacy in deep RL</a></li></ul><h3 id=variable-horizons>Variable horizons</h3><p>In BBF we used a receding update horizon and an increasing discount factor. Why was this useful?</p><blockquote><p>Bias/variance tradeoff!</p></blockquote><p>When using multi-step returns we make a choice of a value of $n$ in the following equation:</p><p>$$Q(x_0, a_0) \leftarrow Q(x_0, a_0) + \alpha \left( \sum_{t=0}^{n}\gamma^t r_t + \max_{a_{n+1}} \gamma^{n+1} Q(x_n, a_{n+1}) - Q(x_0, a_0) \right)$$</p><p>As $n$ grows, this approaches a Monte Carlo estimate which has low bias but high variance; if $n = 0$, it is the standard one-step Bellman update which has low variance but high bias. This notion is theoretically explored in <a href=https://www.cis.upenn.edu/~mkearns/papers/tdlambda.pdf>“Bias-Variance” Error Bounds for Temporal Difference Updates</a>, where the authors suggest a decreasing schedule for $n$, similar to what we used in BBF.</p><p>When using multi-step updates, the choice of the discount factor $\gamma$ affects how much future rewards affect the current state&rsquo;s value estimate. This isillustrated in the following plot, where the $x$-axis indicates the number of steps in the future the reward is received, and the $y$-axis indicates how much it contributes to the value function for different values of $\gamma$.</p><img src=/posts/research/rl/from_bbf_to_sss/value_contribution.png alt="Value contribution" width=50% class=center><p>In BBF we use a schedule that increases $\gamma$, inspired by <a href=https://arxiv.org/abs/1512.02011>How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies</a>.</p><p>How best to balance this bias-variance tradeoff is still an open problem. We identified the benefits certain types of variance can have in <a href=https://arxiv.org/abs/2310.03882>Small batch deep reinforcement learning</a> and in <a href=https://arxiv.org/abs/2406.17523>On the consistency of hyper-parameter selection in value-based deep reinforcement learning</a>.</p><h1 id=why-does-this-matter>Why does this matter?</h1><p>The point of this post is not to promote BBF (although I do quite like it). The main takeaway is:</p><blockquote><p>We designed BBF through experimental, not theoretical, RL.</p></blockquote><p>This is by no means discrediting theoretical RL work, as that is very important. However, the reality is that there is still a theory-practice gap in that theoretical results still tell us very little about how modern methods can/should work. We need to approach deep RL as an <em>empirical science</em>. It needs to be empirical because (unfortunately) theoretical results won&rsquo;t buy us much; and it needs to be scientific in the sense that our aim should be at <em>better understanding how these methods work</em>, as opposed to SotA chasing. I&rsquo;m very encouraged by recent works (in particular those presented at <a href=https://rl-conference.cc/>RLC</a>) that do seem to be pushing in this direction. Let&rsquo;s continue pushing the frontier of empirical science in RL!</p><p>And of course, let&rsquo;s continue pushing the frontiers of theoretical RL research. I like to make a comparison to physics, where there are theoretical and empirical physicists that are <em>complementary</em> to each other, both aiming at a better understanding of how the universe works.</p><img src=/posts/research/rl/from_bbf_to_sss/physics.png alt="Theoretical and empirical physics" width=50% class=center></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/from_bbf_to_sss.md><i class="fas fa-code-branch"></i>
Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/research/rl/redo/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>The Dormant Neuron Phenomenon in Deep Reinforcement Learning</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="does-not-exist",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#bigger-better-faster>Bigger, better, faster</a></li><li><a href=#yes-but-why>Yes, but why?</a><ul><li><a href=#self-supervised-learning>Self-supervised learning</a></li><li><a href=#network-resets>Network resets</a></li><li><a href=#larger-networks>Larger networks</a></li><li><a href=#variable-horizons>Variable horizons</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Selected Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul><a rel=me href=https://sigmoid.social/@psc>Mastodon</a></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad()</script></body></html>