<!doctype html><html><head><title>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/assets/images/psc_emoji.png><link rel=stylesheet href=/assets/css/style.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta name=description content="Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-XXXXXXXXX-X','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/assets/images/psc_emoji.png>psc's website</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/assets/images/psc_emoji.png class=d-none id=main-logo>
<img src=/assets/images/psc_emoji.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/mentoring/>Mentoring / Education</a><ul><li><a href=/posts/mentoring/gridworldplayground/gridworldplayground/>GridWorld Playground</a></li><li><a href=/posts/mentoring/intro-to-rl/intro-to-rl/>Intro to RL</a></li><li><a href=/posts/mentoring/resume/resume/>Preparing your resume</a></li><li><a href=/posts/mentoring/interviewing/interviewing/>Tips for Interviewing at Google</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/art/>Art</a><ul><li><a href=/posts/art/cost-of-beauty/>Cost of Beauty</a></li><li><a href=/posts/art/family/>Family</a></li><li><a href=/posts/art/jidiji/>JiDiJi</a></li><li><a href=/posts/art/musical-aquarium/>Musical Aquarium</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/misc/>Misc</a><ul><li><a href=/posts/misc/agr/>Artificial General Relativity</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/>Research</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/research/other/>Other</a><ul><li><a href=/posts/research/other/rigl/rigl/>RigL</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/research/rl/>RL</a><ul class=active><li><a href=/posts/research/rl/dopamine/>Dopamine</a></li><li><a class=active href=/posts/research/rl/revisiting_rainbow/>Revisiting Rainbow</a></li><li><a href=/posts/research/rl/scalable/>Scalable methods ...</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/research/creativity/>Creativity</a><ul><li><a href=/posts/research/creativity/ganterpretations/>GANterpretations</a></li><li><a href=/posts/research/creativity/ml-jam/>ML-Jam</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://psc-g.github.io/posts/research/rl/revisiting_rainbow/revisiting_rainbow.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/assets/images/psc_gradient.png><h5 class=author-name>Pablo Samuel Castro</h5><p>November 22, 2020</p></div><div class=title><h1>Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</h1></div><div class=post-content id=post-content><p><em>Johan S. Obando-Ceron and Pablo Samuel Castro</em></p><p>This is a summary of our <a href=https://arxiv.org/>paper</a> which will be presented in the
<a href=https://sites.google.com/corp/view/deep-rl-workshop-neurips2020/home>deep reinforcement learning workshop at NeurIPS 2020</a>.</p><h2 id=introduction>Introduction</h2><p>Since the introduction of DQN <a href=https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning>Mnih et al., 2015</a>
reinforcement learning has witnessed a dramatic
increase in research papers <a href=https://arxiv.org/abs/1709.06560>Henderson et al., 2018</a>. A large portion of these papers propose new
methods that build on the original DQN algorithm and network architecture, often adapting methods
introduced before DQN to work well with deep networks. New methods are typically evaluated on a set of environments that have now
become standard, such as the Arcade Learning Environment (ALE)
<a href=https://arxiv.org/abs/1207.4708>Bellemare et al., 2012</a> and the control tasks available in MuJoCo and DM control suites
<a href=https://ieeexplore.ieee.org/document/6386109>Todorov et al., 2012</a>, <a href=https://arxiv.org/abs/2006.12983>Tassa et al., 2020</a>.</p><p>While these benchmarks have helped to evaluate new methods in a standardized manner, they have
also implicitly established a minimum amount of computing power in order to be recognized as
valid scientific contributions.</p><p>Furthermore, at a time when efforts such as <a href=https://blackinai2020.vercel.app/>Black in AI</a> and <a href=http://www.latinxinai.org/>LatinX in AI</a> are helping bring people from
underrepresented (and typically underprivileged) segments of society into the research community,
these newcomers are faced with enormous computational hurdles to overcome if they wish to be an
integral part of said community.</p><p>In this work we argue for a need to change the status-quo in evaluating and proposing new research to avoid
exacerbating the barriers to entry for newcomers from underprivileged communities.</p><p>We complement
this argument by revisiting the Rainbow algorithm <a href=https://arxiv.org/abs/1710.02298>Hessel et al., 2018</a>, which proposed a new state
of the art algorithm by combining a number of recent advances, on a set of small- and medium-sized
tasks. This allows us to conduct a “counterfactual” analysis: would Hessel et al. [2018] have reached
the same conclusions if they had run on the smaller-scale experiments we investigate here? We extend
this analysis by investigating the interaction between the different algorithms considered and the
network architecture used; this is an element not explored by Hessel et al. [2018], yet as we show
below, is crucial for proper evaluation of the methods under consideration.</p><h1 id=the-cost-of-rainbow>The Cost of Rainbow</h1><p>Although the value of the Rainbow agent is undeniable, this result
could have only come from a large research laboratory with ample access to compute:</p><ul><li>It takes roughly 5 days to train an Atari game on a Tesla P100 GPU</li><li>There are 57 games in total</li><li>To report performance with confidence bounds it is common to use at least five independent runs</li></ul><p>Thus, to provide the convincing empirical evidence for Rainbow, Hessel et al. [2018] required
at least 34,200 GPU hours (or 1425 days); in other words, these experiments <em>must</em> be run in parallel with multiple GPUs.</p><p>Considering that the cost of a Tesla P100 is about US<span>$</span>6,000, it becomes prohibitively expensive for
smaller research laboratories. To put things in perspective, the average minimum monthly wage in Latin America
(excluding Venezuela) is approximately US<span>$</span>313 (data taken from <a href=https://www.statista.com/statistics/953880/latin-america-minimum-monthly-wages/>here</a>);
in other words, <em>one GPU is the equivalent of approximately 20 minimum wages.</em></p><p>Needless to say, this is far from inclusive.</p><h2 id=revisiting-rainbow>Revisiting Rainbow</h2><p>As in the original Rainbow paper, we evaluate the effect of adding the following components to the original DQN algorithm:</p><ul><li><strong><a href=https://arxiv.org/abs/1509.06461>Double Q-learning</a></strong> mitigates overestimation bias in the Q-estimates by decoupling the maximization of the action from its selection
in the target bootstrap.</li><li><strong><a href=https://arxiv.org/abs/1511.05952>Prioritized experience replay</a></strong> samples trajectories from the replay buffer proportional to their respective temporal difference error.</li><li><strong><a href=https://arxiv.org/abs/1511.06581>Dueling networks</a></strong> uses two streams sharing the initial convolutional layers, separately estimating $V^*(s)$ and the advantages for each action</li><li><strong><a href=https://link.springer.com/article/10.1007/BF00115009>Multi-step learning</a></strong> uses multi-step targets for temporal difference learning</li><li><strong><a href=https://arxiv.org/abs/1707.06887>Distributional RL</a></strong> maintains estimates of return distributions, rather than return values.</li><li><strong><a href=https://arxiv.org/abs/1706.10295>Noisy Nets</a></strong> replaces standard $\epsilon$-greedy exploration with noisy linear layers that include a noisy stream.</li></ul><h3 id=classic-control>Classic control</h3><p>Our first set of experiments were performed on four classic control environments: CartPole, Acrobot, LunarLander, and MountainCar. We first investigate the effect of independently adding each algorithmic component to DQN:</p><img src=/posts/research/rl/revisiting_rainbow/revisitingClassicAdd.png alt="Revisiting Classic Add"><p>Just like Hessel et al. [2018] we find that, in aggregate, the addition of each of these algorithms does
improve learning over the base DQN. However, while
Hessel et al. [2018] found prioritized replay and multi-step to be the most
impactful additions, in these environments the gains from these additions are
more tempered.
What is most interesting is that when distributional RL is added to DQN <em>without
any of the other components</em>, the gains can sometimes be minimal (see
LunarLander), and can sometimes have a large negative effect on learning
(Acrobot and MountainCar). This is consistent across the various learning rates we considered.</p><p>In contrast, when we look at the <em>removal</em> of each of these components from the full Rainbow agent:</p><img src=/posts/research/rl/revisiting_rainbow/revisitingClassicRemove.png alt="Revisiting Classic Remove"><p>we can see that what hurts the most is the removal of distributional RL. These results suggest there is a symbiotic between distributional RL and one of the other
algorithms considered, an investigation that warrants investigation in future work, along the lines
of the theoretical investigation by <a href=https://arxiv.org/abs/1901.11084>Lyle et al. [2019]</a>, which demonstrated that the combination of
distributional RL with non-linear function approximators can sometimes have adverse effects on
training.</p><h3 id=minatar>MinAtar</h3><p>In order to strengthen the Rainbow Connection, we also ran a set of experiments on the MinAtar
environment <a href=https://arxiv.org/abs/1903.03176>Young and Tian, 2019</a>, which is a set of miniaturized versions of five ALE games
(Asterix, Breakout, Freeway, Seaquest, and SpaceInvaders). These environments are considerably
larger than the four classic control environments previously explored, but they are significantly faster
to train than regular ALE environments. Specifically, training one of these agents takes approximately
12-14 hours on a P100 GPU. For these experiments, we followed the network architecture used by
Young and Tian [2019] consisting of a single convolutional layer followed by a linear layer.</p><img src=/posts/research/rl/revisiting_rainbow/minatar.png alt=MinAtar width=50%></div><div class=btn-improve-page><a href=https://github.com/psc-g/psc-g.github.io/edit/master/content/posts/research/rl/revisiting_rainbow.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/research/creativity/ganterpretations/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>GANterpretations</span></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var dsq=document.createElement("script");dsq.type="text/javascript";dsq.async=true;var disqus_shortname="does-not-exist";dsq.src="//"+disqus_shortname+".disqus.com/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li></ul><ul><li><a href=#revisiting-rainbow>Revisiting Rainbow</a><ul><li><a href=#classic-control>Classic control</a></li><li><a href=#minatar>MinAtar</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span><a href=https://twitter.com/@pcastr target=_blank>Twitter <i class="fab fa-twitter"></i></a></span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>